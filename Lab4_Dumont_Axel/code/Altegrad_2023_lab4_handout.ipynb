{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 4: NLP Frameworks</h2> 07 / 11 / 2023<br> Dr. G. Shang, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> Axel Dumont\n",
        "\n",
        "</center>\n",
        "\n",
        "In this lab you will learn how to use Fairseq and HuggingFace transformers - The most used libraries by researchers and developers  and finetune language models - to finetune a pretrained French language model ($RoBERTa_{small}^{fr}$) on the sentiment analysis dataset CLS_Books where each review is labeled as positive or negative and finetune a variant of BLOOM on a question/answer dataset."
      ],
      "metadata": {
        "id": "DsD-LMKT7XMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Preparing the environment and installing libraries, models and data for Part 1 and Part 2</b>\n",
        "\n",
        "In this section, we will setup the environment on Google Colab (first cell), download the pretraind model (second cell) and the finetuning dataset (third cell). In case you are using your personal computer maket sure to:\n",
        "\n",
        "1- Use Ubuntu (or any similar linux distribution) or MacOS. <b> P.S. In case you have Windows, please use Google Colab. </b>\n",
        "\n",
        "2- <b>Use Anaconda</b> and create new environment if you already installed Fairseq since we will be using a slightly modified version of this library."
      ],
      "metadata": {
        "id": "GwN3KCm5Ec6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir altegrad.lab3 && cd altegrad.lab3 && mkdir libs\n",
        "%cd altegrad.lab3/libs\n",
        "!git clone https://github.com/hadi-abdine/fairseq\n",
        "!pip install git+https://github.com/hadi-abdine/fairseq\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install sentencepiece\n",
        "!pip install tensorboardX\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "6cGqZ9ZB84Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d9ec03-fded-4fc0-e611-8f61e3243a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/altegrad.lab3/libs\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 19737, done.\u001b[K\n",
            "remote: Total 19737 (delta 0), reused 0 (delta 0), pack-reused 19737\u001b[K\n",
            "Receiving objects: 100% (19737/19737), 18.55 MiB | 22.04 MiB/s, done.\n",
            "Resolving deltas: 100% (14319/14319), done.\n",
            "Collecting git+https://github.com/hadi-abdine/fairseq\n",
            "  Cloning https://github.com/hadi-abdine/fairseq to /tmp/pip-req-build-9p1076i1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hadi-abdine/fairseq /tmp/pip-req-build-9p1076i1\n",
            "  Resolved https://github.com/hadi-abdine/fairseq to commit acdd05d6c5c099d7573e199e340df938b5594e99\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.5)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu118)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->fairseq==0.12.2) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->fairseq==0.12.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19006096 sha256=cd4f3457ea14ec43191a2eec765c31dd866ba6aa48f28b48f742a44443163ef3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-81nrnzdl/wheels/32/80/d1/eda3a61a2e0a8d52f2b774f671b0427d1a42d75866252a1515\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=85639e19ef9818a6db8cf2bcda1a005845b77c94420e71cd5d3a3ce43757e128\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.2\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 170002, done.\u001b[K\n",
            "remote: Counting objects: 100% (999/999), done.\u001b[K\n",
            "remote: Compressing objects: 100% (656/656), done.\u001b[K\n",
            "remote: Total 170002 (delta 495), reused 582 (delta 306), pack-reused 169003\u001b[K\n",
            "Receiving objects: 100% (170002/170002), 170.57 MiB | 26.25 MiB/s, done.\n",
            "Resolving deltas: 100% (128247/128247), done.\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-sjgugn37\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-sjgugn37\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 651408a077f842e76e75bfc7d02b8ac38eeb6480\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.36.0.dev0)\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0.dev0)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.36.0.dev0)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8001040 sha256=fb21a989bbccdb20f7745a43611d4781594626057a9002b7d6dbb2471bad7db4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bb4zynr4/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.36.0.dev0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.5\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir models\n",
        "%cd ../models\n",
        "\n",
        "!wget -c 'https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip' -O \"model_fairseq.zip\"\n",
        "!unzip model_fairseq.zip\n",
        "!rm model_fairseq.zip\n",
        "!rm -rf __MACOSX/\n",
        "\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\" -O \"model_huggingface.zip\"\n",
        "!unzip model_huggingface.zip\n",
        "!rm model_huggingface.zip\n",
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "rxHffsPm-EqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba19027-393b-450b-9af9-8150bcfff081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/altegrad.lab3/models\n",
            "--2023-11-16 16:55:20--  https://nuage.lix.polytechnique.fr/index.php/s/E4otcD7B9jm2AWx/download/RoBERTa_small_fr.zip\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264887103 (253M) [application/zip]\n",
            "Saving to: ‘model_fairseq.zip’\n",
            "\n",
            "model_fairseq.zip   100%[===================>] 252.62M  26.1MB/s    in 10s     \n",
            "\n",
            "2023-11-16 16:55:31 (24.4 MB/s) - ‘model_fairseq.zip’ saved [264887103/264887103]\n",
            "\n",
            "Archive:  model_fairseq.zip\n",
            "   creating: RoBERTa_small_fr/\n",
            "  inflating: RoBERTa_small_fr/model.pt  \n",
            "  inflating: __MACOSX/RoBERTa_small_fr/._model.pt  \n",
            "  inflating: RoBERTa_small_fr/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr/dict.txt  \n",
            "--2023-11-16 16:55:37--  https://nuage.lix.polytechnique.fr/index.php/s/yYQjg9XWekttG5j/download/RoBERTa_small_fr_HuggingFace.zip\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53984187 (51M) [application/zip]\n",
            "Saving to: ‘model_huggingface.zip’\n",
            "\n",
            "model_huggingface.z 100%[===================>]  51.48M  19.4MB/s    in 2.7s    \n",
            "\n",
            "2023-11-16 16:55:40 (19.4 MB/s) - ‘model_huggingface.zip’ saved [53984187/53984187]\n",
            "\n",
            "Archive:  model_huggingface.zip\n",
            "   creating: RoBERTa_small_fr_HuggingFace/\n",
            "  inflating: RoBERTa_small_fr_HuggingFace/sentencepiece.bpe.model  \n",
            "  inflating: RoBERTa_small_fr_HuggingFace/config.json  \n",
            "  inflating: RoBERTa_small_fr_HuggingFace/pytorch_model.bin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd .. && mkdir data\n",
        "%cd ../data\n",
        "!wget -c \"https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\" -O \"cls.books.zip\"\n",
        "!unzip cls.books.zip\n",
        "!rm cls.books.zip\n",
        "!rm -rf __MACOSX/\n",
        "!mkdir cls.books-json\n",
        "%cd .."
      ],
      "metadata": {
        "id": "HfZ_znATNdya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8211d303-01ed-4476-955f-b8afc5646c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/altegrad.lab3/data\n",
            "--2023-11-16 16:55:44--  https://nuage.lix.polytechnique.fr/index.php/s/EBHqfR776oCE2Nj/download/cls.books.zip\n",
            "Resolving nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)... 193.55.176.11\n",
            "Connecting to nuage.lix.polytechnique.fr (nuage.lix.polytechnique.fr)|193.55.176.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 955190 (933K) [application/zip]\n",
            "Saving to: ‘cls.books.zip’\n",
            "\n",
            "cls.books.zip       100%[===================>] 932.80K  1.47MB/s    in 0.6s    \n",
            "\n",
            "2023-11-16 16:55:45 (1.47 MB/s) - ‘cls.books.zip’ saved [955190/955190]\n",
            "\n",
            "Archive:  cls.books.zip\n",
            "   creating: cls.books/\n",
            "  inflating: cls.books/valid.label   \n",
            "  inflating: cls.books/test.review   \n",
            "  inflating: cls.books/valid.review  \n",
            "  inflating: cls.books/train.review  \n",
            "  inflating: cls.books/train.label   \n",
            "  inflating: cls.books/test.label    \n",
            "/content/altegrad.lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 1: Fairseq</b>\n",
        "\n",
        "In the first part of this lab, you will finetune the given model on model on CLS_Books dataset using <b>Fairseq</b> by following these steps:<br>\n",
        "\n",
        " 1- <b>Tokenize the reviews</b> (Train, Valid and Test) using trained sentencepiece tokenizer provided alongside the pretrained model.[using sentencepiece library and setting the parameter <b>out_type=str</b> in the encode function].<br>\n",
        " 2- <b>Binarize the tokenized reviews and their labels</b> using the preprocess python script provided in Fairseq.<br>\n",
        " 3- <b>Fintune the pretrained $RoBERTa_{small}^{fr}$ model</b> using the train python script provided in Fairseq.<br>\n",
        "\n",
        " Finally, you will finish the first part by training a random $RoBERTa_{small}^{fr}$ model on the CLS_Books dataset and compare the results against the pretrained model while <b>visualizing the accuracies on tensorboard</b>."
      ],
      "metadata": {
        "id": "H40TxVIvEWyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b> Number of parameters of the model</b>\n",
        "\n",
        "In this section you have to compute the number of parameters of $RoBERTa_{small}^{fr}$ using PyTorch (Only the base model with out the head). (<b>Hint:</b> you can check the architecture of the model using model['model'])"
      ],
      "metadata": {
        "id": "TvpyZEexOXHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.load(\"models/RoBERTa_small_fr/model.pt\")\n",
        "\n",
        "params = None\n",
        "num_total = 0\n",
        "for key in model['model'].keys():\n",
        "  if 'bias' not in key and 'head' not in key and 'version' not in key and 'norm' not in key:\n",
        "    params = model['model'][key].size()\n",
        "    num_total += model['model'][key].numel()\n",
        "    print(f'{key} : {params}')\n",
        "\n",
        "print(\"\\n The total number of parameters: \", num_total) #fill the gap"
      ],
      "metadata": {
        "id": "j7isz60LOwlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c535c63a-339e-4177-b8b1-f9288210fec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder.sentence_encoder.embed_tokens.weight : torch.Size([32000, 512])\n",
            "encoder.sentence_encoder.embed_positions.weight : torch.Size([258, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.k_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.v_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.q_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.self_attn.out_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.fc1.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.0.fc2.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.k_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.v_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.q_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.self_attn.out_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.fc1.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.1.fc2.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.k_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.v_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.q_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.self_attn.out_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.fc1.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.2.fc2.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.k_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.v_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.q_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.self_attn.out_proj.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.fc1.weight : torch.Size([512, 512])\n",
            "encoder.sentence_encoder.layers.3.fc2.weight : torch.Size([512, 512])\n",
            "\n",
            " The total number of parameters:  22807552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Tokenizing the reviews</b>\n",
        "\n",
        "In this section we will tokenize the finetuning dataset using sentenpiece tokenizer. We have three splits in our datase: train valid and test sets.\n",
        "\n",
        "In this task you have to use the trained sentencepiece tokenizer (RoBERTa_small_fr/sentencepiece.bpe.model) to tokenize the three files <b>train.review</b>, <b>valid.review</b> and <b>test.review</b> and output the three files <b>train.spm.review</b>, <b>valid.spm.review</b> and <b>test.spm.review</b> containing the tokenized reviews.\n",
        "\n",
        "Documentation: https://github.com/google/sentencepiece#readme"
      ],
      "metadata": {
        "id": "gz8fnWOSI0eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "s = spm.SentencePieceProcessor(model_file='models/RoBERTa_small_fr/sentencepiece.bpe.model')\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "SENTS=\"review\"\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.'+SENTS, 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "        reviews = [s.encode(review.strip(), out_type=str) for review in reviews] # tokenize the data using s.encode and a loop(check the documentation)\n",
        "        # It should look something like this :\n",
        "        #▁An ci enne ▁VS ▁Nouvelle ▁version ▁plus\n",
        "\n",
        "    with open('data/cls.books/'+split+'.spm.'+SENTS, 'w') as f:\n",
        "        for review in reviews:\n",
        "          f.write(' '.join(review) + '\\n')"
      ],
      "metadata": {
        "id": "D-hOlotmOW7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Binarizing/Preprocessing the finetuning dataset</b>\n",
        "\n",
        "In this section, you have to binarize the CLS_Books dataset using the <b>fairseq/fairseq_cli/preprocess.py</b> script:\n",
        "\n",
        "1- Binarize the tokenized reviews and put the output in <b>data/cls-books-bin/input0</b>. Note: Our pretrained model's embedding matrix contains only the embedding of the vocab listed in the dictionary <b>dict.txt</b>. You need to use the dictionary in the binarization of the text to transform the tokens into indices. Also note that we are using Encoder only architecture, so we only have source data.\n",
        "\n",
        "2- Binarize the labels (train.label, valid.label and test.label files) and put the output in <b>data/cls-books-bin/label</b>.\n",
        "\n",
        "Documentation: https://fairseq.readthedocs.io/en/latest/command_line_tools.html"
      ],
      "metadata": {
        "id": "iGNK19XuKBk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "              --trainpref data/cls.books/train.spm.review \\\n",
        "              --validpref data/cls.books/valid.spm.review \\\n",
        "              --testpref data/cls.books/test.spm.review \\\n",
        "              --destdir data/cls-books-bin/input0 \\\n",
        "              --workers 8 \\\n",
        "              --srcdict models/RoBERTa_small_fr/dict.txt)#fill me - binarize the tokenized reviews\n",
        "\n",
        "!(python libs/fairseq/fairseq_cli/preprocess.py \\\n",
        "              --only-source \\\n",
        "              --trainpref data/cls.books/train.label \\\n",
        "              --validpref data/cls.books/valid.label \\\n",
        "              --testpref data/cls.books/test.label\\\n",
        "              --destdir data/cls-books-bin/label\\\n",
        "              --workers 8) #fill me - binarize the labels"
      ],
      "metadata": {
        "id": "jIF1wvWoFp4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785a16b0-0bf5-4165-aba2-5b877275ce16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 16:56:00.407290: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 16:56:00.407351: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 16:56:00.407395: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 16:56:00.420321: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 16:56:02.676196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 16:56:11 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='data/cls.books/train.spm.review', validpref='data/cls.books/valid.spm.review', testpref='data/cls.books/test.spm.review', align_suffix=None, destdir='data/cls-books-bin/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='models/RoBERTa_small_fr/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
            "2023-11-16 16:56:11 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
            "2023-11-16 16:56:16 | INFO | fairseq_cli.preprocess | [None] data/cls.books/train.spm.review: 1800 sents, 284877 tokens, 0.13% replaced (by <unk>)\n",
            "2023-11-16 16:56:16 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
            "2023-11-16 16:56:16 | INFO | fairseq_cli.preprocess | [None] data/cls.books/valid.spm.review: 200 sents, 30354 tokens, 0.135% replaced (by <unk>)\n",
            "2023-11-16 16:56:16 | INFO | fairseq_cli.preprocess | [None] Dictionary: 31999 types\n",
            "2023-11-16 16:56:19 | INFO | fairseq_cli.preprocess | [None] data/cls.books/test.spm.review: 2000 sents, 311660 tokens, 0.139% replaced (by <unk>)\n",
            "2023-11-16 16:56:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/cls-books-bin/input0\n",
            "2023-11-16 16:56:24.623176: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 16:56:24.623229: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 16:56:24.623266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 16:56:24.630617: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 16:56:25.720483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 16:56:28 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='data/cls.books/train.label', validpref='data/cls.books/valid.label', testpref='data/cls.books/test.label', align_suffix=None, destdir='data/cls-books-bin/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=8, dict_only=False)\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | [None] data/cls.books/train.label: 1800 sents, 3600 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | [None] data/cls.books/valid.label: 200 sents, 400 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | [None] data/cls.books/test.label: 2000 sents, 4000 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-16 16:56:29 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/cls-books-bin/label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$</b>\n",
        "\n",
        "In this section you will use <b>fairseq/fairseq_cli/train.py</b> python script to finetune the pretrained model on the CLS_Books dataset (binarized data) for three different seeds: 0, 1 and 2.\n",
        "\n",
        "Make sure to use the following hyper-parameters: $\\textit{batch size}=8, \\textit{max number of epochs}: 5, \\textit{optimizer}: Adam, \\textit{max learning rate}: 1e-05,  \\textit{warm up ratio}: 0.06, \\textit{learning rate scheduler}: linear$"
      ],
      "metadata": {
        "id": "2SSjBcQJnuSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "TASK='sentence_prediction' # fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr'\n",
        "DATA_PATH='data/cls-books-bin' # fill me\n",
        "MODEL_PATH='models/RoBERTa_small_fr/model.pt' # fill me\n",
        "MAX_EPOCH=5 # fill me\n",
        "MAX_SENTENCES=8 # fill me, batch size\n",
        "MAX_UPDATE=3125  # fill me, number of backward propagation steps\n",
        "LR=1e-5 # fill me\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC='accuracy' # fill me, use the accuracy metric\n",
        "NUM_CLASSES=2 #fill me, number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP=187 # fill me, warmup ratio=6% of the whole training"
      ],
      "metadata": {
        "id": "JV2112YPJEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $WARMUP \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)\n"
      ],
      "metadata": {
        "id": "6Mdznms-EYyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5a7a62-70d4-4f62-aaaa-581b07fd5783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 16:56:34.403804: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 16:56:34.403873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 16:56:34.403913: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 16:56:34.415155: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 16:56:36.113709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 16:56:37 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 3125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0', 'restore_file': 'models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=3125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0', restore_file='models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=187, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='3125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 187, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 3125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-16 16:56:40 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-16 16:56:40 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-16 16:56:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-16 16:56:40 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/input0/valid\n",
            "2023-11-16 16:56:40 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/label/valid\n",
            "2023-11-16 16:56:40 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-16 16:56:40 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/input0/test\n",
            "2023-11-16 16:56:40 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/label/test\n",
            "2023-11-16 16:56:40 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-16 16:56:48 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-16 16:56:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 16:56:48 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-16 16:56:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 16:56:48 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-16 16:56:48 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-16 16:56:48 | INFO | fairseq.trainer | Preparing to load checkpoint models/RoBERTa_small_fr/model.pt\n",
            "2023-11-16 16:56:48 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
            "2023-11-16 16:56:48 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
            "2023-11-16 16:56:48 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
            "2023-11-16 16:56:48 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
            "2023-11-16 16:56:48 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-16 16:56:49 | INFO | fairseq.trainer | Loaded checkpoint models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
            "2023-11-16 16:56:49 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-16 16:56:49 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/input0/train\n",
            "2023-11-16 16:56:49 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/label/train\n",
            "2023-11-16 16:56:49 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-16 16:56:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:56:49 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-16 16:56:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:14<00:00, 21.71it/s, loss=0.903, nll_loss=0.007, accuracy=85, wps=23695, ups=22.09, wpb=1072.6, bsz=8, num_updates=220, lr=9.88768e-06, gnorm=3.504, train_wall=0, gb_free=14.2, wall=15]2023-11-16 16:57:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 46.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 59.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:57:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.958 | nll_loss 0.007 | accuracy 57 | wps 77843.9 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-16 16:57:04 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   1% 3/250 [00:00<00:10, 24.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 43.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 17/250 [00:00<00:04, 57.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 64.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 68.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 70.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 72.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 73.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 73.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 74.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 73.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 66.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 64.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 61.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  44% 110/250 [00:01<00:02, 59.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  47% 117/250 [00:01<00:02, 60.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 124/250 [00:01<00:02, 60.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 131/250 [00:02<00:01, 59.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  55% 138/250 [00:02<00:01, 59.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 59.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 59.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  63% 157/250 [00:02<00:01, 59.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  66% 164/250 [00:02<00:01, 60.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  68% 171/250 [00:02<00:01, 60.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  71% 178/250 [00:02<00:01, 59.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  74% 185/250 [00:02<00:01, 60.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  77% 192/250 [00:03<00:00, 59.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 59.44it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  82% 204/250 [00:03<00:00, 59.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  84% 210/250 [00:03<00:00, 59.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 216/250 [00:03<00:00, 58.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 58.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  91% 228/250 [00:03<00:00, 56.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  94% 234/250 [00:03<00:00, 56.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  96% 240/250 [00:03<00:00, 55.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 54.12it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:57:08 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.936 | nll_loss 0.007 | accuracy 60.5 | wps 65067 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-16 16:57:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-16 16:57:08 | INFO | train | epoch 001 | loss 0.994 | nll_loss 0.007 | accuracy 53.2 | wps 15181.1 | ups 14.22 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 9.87066e-06 | gnorm 3.364 | train_wall 14 | gb_free 14.3 | wall 20\n",
            "2023-11-16 16:57:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:57:08 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-16 16:57:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 222/225 [00:11<00:00, 21.44it/s, loss=0.809, nll_loss=0.005, accuracy=70, wps=25531.5, ups=21.6, wpb=1182.2, bsz=8, num_updates=445, lr=9.12185e-06, gnorm=7.454, train_wall=0, gb_free=14.3, wall=32]2023-11-16 16:57:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 40.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 59.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:57:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.903 | nll_loss 0.007 | accuracy 65 | wps 75835.1 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 65\n",
            "2023-11-16 16:57:21 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   1% 2/250 [00:00<00:15, 15.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 42.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:03, 58.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 64.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 68.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 71.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 72.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 73.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 73.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 73.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 74.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 74.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 74.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 105/250 [00:01<00:01, 73.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 113/250 [00:01<00:01, 73.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 121/250 [00:01<00:01, 73.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 129/250 [00:01<00:01, 72.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 137/250 [00:01<00:01, 72.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 73.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 73.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 161/250 [00:02<00:01, 74.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  68% 169/250 [00:02<00:01, 74.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  71% 177/250 [00:02<00:00, 73.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 185/250 [00:02<00:00, 72.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  77% 193/250 [00:02<00:00, 71.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  81% 202/250 [00:02<00:00, 74.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 210/250 [00:02<00:00, 73.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  87% 218/250 [00:03<00:00, 73.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  90% 226/250 [00:03<00:00, 74.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  94% 234/250 [00:03<00:00, 74.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  97% 242/250 [00:03<00:00, 73.52it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:57:24 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.838 | nll_loss 0.006 | accuracy 70.2 | wps 77038.4 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-16 16:57:24 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-16 16:57:24 | INFO | train | epoch 002 | loss 0.801 | nll_loss 0.006 | accuracy 73.1 | wps 14950.5 | ups 14.01 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 9.10483e-06 | gnorm 5.846 | train_wall 11 | gb_free 14.3 | wall 36\n",
            "2023-11-16 16:57:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:57:24 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-16 16:57:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 222/225 [00:14<00:00, 20.77it/s, loss=0.748, nll_loss=0.005, accuracy=72.5, wps=23150.1, ups=21.11, wpb=1096.6, bsz=8, num_updates=670, lr=8.35602e-06, gnorm=7.448, train_wall=0, gb_free=14.3, wall=51]2023-11-16 16:57:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 16.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 43.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 58.07it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:57:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.588 | nll_loss 0.004 | accuracy 81 | wps 74028.5 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 81\n",
            "2023-11-16 16:57:39 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 40.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 57.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 65.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 69.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 40/250 [00:00<00:02, 72.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  19% 48/250 [00:00<00:02, 72.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  22% 56/250 [00:00<00:02, 73.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  26% 64/250 [00:00<00:02, 73.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  29% 72/250 [00:01<00:02, 74.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  32% 80/250 [00:01<00:02, 72.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 88/250 [00:01<00:02, 73.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 74.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  42% 104/250 [00:01<00:01, 73.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  45% 112/250 [00:01<00:01, 71.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  48% 120/250 [00:01<00:01, 71.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  51% 128/250 [00:01<00:01, 72.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 136/250 [00:01<00:01, 73.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 73.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 74.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 160/250 [00:02<00:01, 74.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  67% 168/250 [00:02<00:01, 75.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  70% 176/250 [00:02<00:00, 75.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  74% 184/250 [00:02<00:00, 72.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  77% 192/250 [00:02<00:00, 73.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  80% 200/250 [00:02<00:00, 74.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  83% 208/250 [00:02<00:00, 73.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  86% 216/250 [00:03<00:00, 72.63it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  90% 224/250 [00:03<00:00, 72.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  93% 232/250 [00:03<00:00, 71.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  96% 240/250 [00:03<00:00, 72.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset: 100% 249/250 [00:03<00:00, 75.37it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:57:43 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.647 | nll_loss 0.005 | accuracy 80 | wps 76848.5 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-16 16:57:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-16 16:57:43 | INFO | train | epoch 003 | loss 0.643 | nll_loss 0.005 | accuracy 79.1 | wps 12935.8 | ups 12.12 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 8.33901e-06 | gnorm 7.841 | train_wall 13 | gb_free 14.3 | wall 55\n",
            "2023-11-16 16:57:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:57:43 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-16 16:57:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 18.06it/s, loss=0.488, nll_loss=0.003, accuracy=87.5, wps=20103.8, ups=17.37, wpb=1157.6, bsz=8, num_updates=895, lr=7.5902e-06, gnorm=8.249, train_wall=0, gb_free=14.3, wall=67]2023-11-16 16:57:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 26.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 35.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 19/25 [00:00<00:00, 50.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 52.77it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:57:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.643 | nll_loss 0.005 | accuracy 77 | wps 60400.4 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 81\n",
            "2023-11-16 16:57:56 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   0% 1/250 [00:00<00:30,  8.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   2% 4/250 [00:00<00:12, 19.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 10/250 [00:00<00:06, 35.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   6% 16/250 [00:00<00:05, 43.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   9% 22/250 [00:00<00:04, 48.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 49.51it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 50.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 51.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  18% 46/250 [00:01<00:03, 51.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 51.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 52.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 54.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 56.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 55.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  34% 84/250 [00:01<00:03, 51.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  36% 90/250 [00:01<00:03, 51.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 96/250 [00:01<00:03, 50.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  41% 102/250 [00:02<00:02, 51.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  43% 108/250 [00:02<00:02, 53.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 53.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 52.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 53.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:02<00:01, 59.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 62.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 66.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  63% 157/250 [00:02<00:01, 66.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 165/250 [00:03<00:01, 67.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  69% 172/250 [00:03<00:01, 60.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  72% 180/250 [00:03<00:01, 64.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  75% 188/250 [00:03<00:00, 66.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  78% 196/250 [00:03<00:00, 68.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  82% 204/250 [00:03<00:00, 70.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  85% 212/250 [00:03<00:00, 70.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 71.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  91% 228/250 [00:03<00:00, 71.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  94% 236/250 [00:04<00:00, 71.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 244/250 [00:04<00:00, 72.73it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:58:00 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.662 | nll_loss 0.005 | accuracy 79.3 | wps 62775.1 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-16 16:58:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-16 16:58:00 | INFO | train | epoch 004 | loss 0.557 | nll_loss 0.004 | accuracy 83.5 | wps 14076.1 | ups 13.19 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 7.57318e-06 | gnorm 8.715 | train_wall 11 | gb_free 14.3 | wall 72\n",
            "2023-11-16 16:58:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:58:00 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-16 16:58:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:11<00:00, 17.76it/s, loss=0.357, nll_loss=0.003, accuracy=87.5, wps=18648.5, ups=16.7, wpb=1117, bsz=8, num_updates=1120, lr=6.82437e-06, gnorm=11.579, train_wall=0, gb_free=14.3, wall=83]2023-11-16 16:58:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  16% 4/25 [00:00<00:01, 19.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  40% 10/25 [00:00<00:00, 35.79it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 42.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 23/25 [00:00<00:00, 51.48it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:58:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.514 | nll_loss 0.004 | accuracy 85.5 | wps 55978.9 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 85.5\n",
            "2023-11-16 16:58:12 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:34,  7.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 36.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 43.44it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 47.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 49.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 51.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 41/250 [00:00<00:04, 51.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  19% 47/250 [00:01<00:04, 50.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 50.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 51.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 53.44it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 53.41it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 52.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 52.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  36% 89/250 [00:01<00:03, 53.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 52.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  40% 101/250 [00:02<00:02, 53.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 53.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 53.69it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 53.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 53.01it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 53.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 52.47it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 54.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 55.62it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  62% 156/250 [00:03<00:01, 57.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  65% 162/250 [00:03<00:01, 56.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  68% 169/250 [00:03<00:01, 57.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 175/250 [00:03<00:01, 57.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  72% 181/250 [00:03<00:01, 57.92it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  75% 188/250 [00:03<00:01, 58.43it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  78% 194/250 [00:03<00:00, 56.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 200/250 [00:03<00:00, 56.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  82% 206/250 [00:03<00:00, 56.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  85% 212/250 [00:04<00:00, 54.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  87% 218/250 [00:04<00:00, 55.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  90% 224/250 [00:04<00:00, 55.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 230/250 [00:04<00:00, 55.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 57.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 244/250 [00:04<00:00, 58.89it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:58:17 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.631 | nll_loss 0.005 | accuracy 81.4 | wps 57341.7 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-16 16:58:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-16 16:58:17 | INFO | train | epoch 005 | loss 0.491 | nll_loss 0.004 | accuracy 86 | wps 14093.9 | ups 13.21 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 6.80735e-06 | gnorm 9.732 | train_wall 11 | gb_free 14.3 | wall 89\n",
            "2023-11-16 16:58:17 | INFO | fairseq_cli.train | done training in 87.9 seconds\n",
            "2023-11-16 16:58:21.456190: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 16:58:21.456247: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 16:58:21.456283: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 16:58:21.464003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 16:58:22.583041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 16:58:23 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-16 16:58:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 3125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1', 'restore_file': 'models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=3125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1', restore_file='models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=187, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='3125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 187, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 3125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-16 16:58:26 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-16 16:58:26 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-16 16:58:27 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-16 16:58:27 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-16 16:58:27 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-16 16:58:27 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-16 16:58:27 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-16 16:58:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-16 16:58:27 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/input0/valid\n",
            "2023-11-16 16:58:27 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/label/valid\n",
            "2023-11-16 16:58:27 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-16 16:58:27 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/input0/test\n",
            "2023-11-16 16:58:27 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/label/test\n",
            "2023-11-16 16:58:27 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-16 16:58:29 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-16 16:58:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 16:58:29 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-16 16:58:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 16:58:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-16 16:58:29 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-16 16:58:29 | INFO | fairseq.trainer | Preparing to load checkpoint models/RoBERTa_small_fr/model.pt\n",
            "2023-11-16 16:58:30 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
            "2023-11-16 16:58:30 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
            "2023-11-16 16:58:30 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
            "2023-11-16 16:58:30 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
            "2023-11-16 16:58:30 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-16 16:58:30 | INFO | fairseq.trainer | Loaded checkpoint models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
            "2023-11-16 16:58:30 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-16 16:58:30 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/input0/train\n",
            "2023-11-16 16:58:30 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/label/train\n",
            "2023-11-16 16:58:30 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-16 16:58:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:58:30 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-16 16:58:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 224/225 [00:13<00:00, 20.48it/s, loss=0.902, nll_loss=0.007, accuracy=65, wps=21930.5, ups=21.15, wpb=1037, bsz=8, num_updates=220, lr=9.88768e-06, gnorm=3.526, train_wall=0, gb_free=14.3, wall=15]2023-11-16 16:58:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 41.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 55.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 66.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:58:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.937 | nll_loss 0.007 | accuracy 62 | wps 73462.7 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-16 16:58:45 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   3% 7/250 [00:00<00:06, 36.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   5% 13/250 [00:00<00:05, 46.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   8% 20/250 [00:00<00:04, 53.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  11% 28/250 [00:00<00:03, 60.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 36/250 [00:00<00:03, 64.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  18% 44/250 [00:00<00:02, 69.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  21% 52/250 [00:00<00:02, 70.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 60/250 [00:00<00:02, 72.74it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  27% 68/250 [00:01<00:02, 72.24it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  30% 76/250 [00:01<00:02, 73.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 84/250 [00:01<00:02, 71.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 71.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  40% 100/250 [00:01<00:02, 72.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 108/250 [00:01<00:01, 72.55it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  46% 116/250 [00:01<00:01, 72.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 124/250 [00:01<00:01, 73.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  53% 132/250 [00:01<00:01, 72.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  56% 140/250 [00:02<00:01, 71.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  59% 148/250 [00:02<00:01, 71.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  62% 156/250 [00:02<00:01, 70.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  66% 164/250 [00:02<00:01, 72.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  69% 172/250 [00:02<00:01, 71.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  72% 180/250 [00:02<00:00, 71.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  75% 188/250 [00:02<00:00, 71.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  78% 196/250 [00:02<00:00, 70.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  82% 204/250 [00:02<00:00, 70.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  85% 212/250 [00:03<00:00, 71.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 70.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  91% 228/250 [00:03<00:00, 69.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  94% 236/250 [00:03<00:00, 70.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  98% 244/250 [00:03<00:00, 71.01it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:58:48 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.901 | nll_loss 0.007 | accuracy 68.8 | wps 73996.7 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-16 16:58:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-16 16:58:48 | INFO | train | epoch 001 | loss 0.989 | nll_loss 0.007 | accuracy 55.4 | wps 14147.2 | ups 13.27 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 9.87066e-06 | gnorm 3.384 | train_wall 13 | gb_free 14.3 | wall 19\n",
            "2023-11-16 16:58:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:58:48 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-16 16:58:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 224/225 [00:12<00:00, 19.09it/s, loss=0.69, nll_loss=0.005, accuracy=77.5, wps=18601.4, ups=18.28, wpb=1017.4, bsz=8, num_updates=445, lr=9.12185e-06, gnorm=7.491, train_wall=0, gb_free=14.2, wall=31]2023-11-16 16:59:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 40.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 54.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 62.97it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:59:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.778 | nll_loss 0.006 | accuracy 69 | wps 69777.4 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 69\n",
            "2023-11-16 16:59:02 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   1% 2/250 [00:00<00:15, 16.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 43.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:04, 56.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 63.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 65.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 41/250 [00:00<00:03, 69.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 70.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 68.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 65/250 [00:01<00:02, 69.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 70.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 69.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 71.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 70.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 105/250 [00:01<00:02, 70.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 113/250 [00:01<00:01, 71.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 121/250 [00:01<00:01, 71.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 129/250 [00:01<00:01, 71.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 137/250 [00:02<00:01, 70.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 71.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 70.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 161/250 [00:02<00:01, 71.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  68% 169/250 [00:02<00:01, 72.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  71% 177/250 [00:02<00:01, 71.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 185/250 [00:02<00:00, 70.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  77% 193/250 [00:02<00:00, 69.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  80% 200/250 [00:02<00:00, 67.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  83% 208/250 [00:03<00:00, 69.15it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 216/250 [00:03<00:00, 70.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  90% 224/250 [00:03<00:00, 69.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  93% 232/250 [00:03<00:00, 69.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 240/250 [00:03<00:00, 69.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset: 100% 249/250 [00:03<00:00, 73.21it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:59:05 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.737 | nll_loss 0.006 | accuracy 75.6 | wps 73656.3 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-16 16:59:05 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-16 16:59:05 | INFO | train | epoch 002 | loss 0.794 | nll_loss 0.006 | accuracy 73.6 | wps 14157.1 | ups 13.27 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 9.10483e-06 | gnorm 6.32 | train_wall 12 | gb_free 14.3 | wall 36\n",
            "2023-11-16 16:59:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:59:05 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-16 16:59:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 222/225 [00:12<00:00, 16.85it/s, loss=0.614, nll_loss=0.004, accuracy=85, wps=19494.7, ups=17.51, wpb=1113.4, bsz=8, num_updates=670, lr=8.35602e-06, gnorm=9.476, train_wall=0, gb_free=14.3, wall=48]2023-11-16 16:59:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.65it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 4/25 [00:00<00:01, 19.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 32.00it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 39.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 20/25 [00:00<00:00, 42.53it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:59:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.608 | nll_loss 0.005 | accuracy 80.5 | wps 49751.2 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 80.5\n",
            "2023-11-16 16:59:18 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:31,  7.90it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 36.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 41.95it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   9% 22/250 [00:00<00:05, 44.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 46.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 47.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 48.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  18% 46/250 [00:01<00:04, 50.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 50.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 51.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 51.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 52.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 76/250 [00:01<00:03, 52.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  33% 82/250 [00:01<00:03, 52.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 88/250 [00:01<00:03, 52.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 52.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  40% 100/250 [00:02<00:02, 51.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  42% 106/250 [00:02<00:02, 53.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  45% 112/250 [00:02<00:02, 52.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 118/250 [00:02<00:02, 53.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  50% 124/250 [00:02<00:02, 53.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  52% 130/250 [00:02<00:02, 55.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 136/250 [00:02<00:02, 54.32it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 54.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  59% 148/250 [00:02<00:01, 55.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  62% 154/250 [00:03<00:01, 55.38it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 160/250 [00:03<00:01, 55.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  66% 166/250 [00:03<00:01, 56.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  69% 172/250 [00:03<00:01, 55.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  71% 178/250 [00:03<00:01, 55.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  74% 184/250 [00:03<00:01, 52.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  76% 190/250 [00:03<00:01, 53.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  78% 196/250 [00:03<00:00, 54.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  82% 204/250 [00:03<00:00, 58.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  85% 212/250 [00:04<00:00, 62.49it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 219/250 [00:04<00:00, 64.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 66.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  94% 235/250 [00:04<00:00, 68.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  97% 243/250 [00:04<00:00, 69.72it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:59:23 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.645 | nll_loss 0.005 | accuracy 79.4 | wps 58632.5 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-16 16:59:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-16 16:59:23 | INFO | train | epoch 003 | loss 0.651 | nll_loss 0.005 | accuracy 78.8 | wps 13687.1 | ups 12.83 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 8.33901e-06 | gnorm 7.846 | train_wall 11 | gb_free 14.3 | wall 53\n",
            "2023-11-16 16:59:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:59:23 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-16 16:59:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 223/225 [00:11<00:00, 18.61it/s, loss=0.562, nll_loss=0.004, accuracy=82.5, wps=19949.4, ups=19.93, wpb=1001, bsz=8, num_updates=895, lr=7.5902e-06, gnorm=8.028, train_wall=0, gb_free=14.3, wall=65]2023-11-16 16:59:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 24.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 38.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 42.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 51.03it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:59:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.564 | nll_loss 0.004 | accuracy 81 | wps 56189.6 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 81\n",
            "2023-11-16 16:59:35 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   0% 1/250 [00:00<00:33,  7.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   2% 6/250 [00:00<00:08, 28.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 37.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 45.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 52.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 54.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  14% 36/250 [00:00<00:03, 54.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  17% 43/250 [00:00<00:03, 56.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  20% 49/250 [00:00<00:03, 56.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 55/250 [00:01<00:03, 54.28it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  24% 61/250 [00:01<00:03, 54.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  27% 67/250 [00:01<00:03, 53.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  29% 73/250 [00:01<00:03, 52.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 55.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 56.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 56.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  39% 98/250 [00:01<00:02, 55.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  42% 104/250 [00:01<00:02, 54.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 110/250 [00:02<00:02, 54.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  46% 116/250 [00:02<00:02, 53.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  49% 122/250 [00:02<00:02, 53.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 52.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:02<00:02, 52.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 55.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 54.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 55.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  64% 159/250 [00:03<00:01, 55.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 165/250 [00:03<00:01, 55.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  68% 171/250 [00:03<00:01, 55.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  71% 177/250 [00:03<00:01, 54.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 183/250 [00:03<00:01, 54.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 189/250 [00:03<00:01, 53.14it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  78% 195/250 [00:03<00:01, 53.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 52.90it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 53.41it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  85% 213/250 [00:04<00:00, 53.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 219/250 [00:04<00:00, 53.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  90% 225/250 [00:04<00:00, 53.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  92% 231/250 [00:04<00:00, 53.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  95% 238/250 [00:04<00:00, 56.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 58.52it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:59:40 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.626 | nll_loss 0.005 | accuracy 81.1 | wps 57711.4 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-16 16:59:40 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-16 16:59:40 | INFO | train | epoch 004 | loss 0.557 | nll_loss 0.004 | accuracy 83.2 | wps 14157.4 | ups 13.27 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 7.57318e-06 | gnorm 8.876 | train_wall 11 | gb_free 14.3 | wall 70\n",
            "2023-11-16 16:59:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-16 16:59:40 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-16 16:59:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 224/225 [00:11<00:00, 20.41it/s, loss=0.399, nll_loss=0.003, accuracy=85, wps=20094.5, ups=21.08, wpb=953.4, bsz=8, num_updates=1120, lr=6.82437e-06, gnorm=7.945, train_wall=0, gb_free=14.3, wall=82]2023-11-16 16:59:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 41.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 54.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 65.96it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 16:59:52 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.539 | nll_loss 0.004 | accuracy 84.5 | wps 72204.3 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 84.5\n",
            "2023-11-16 16:59:52 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   3% 8/250 [00:00<00:06, 40.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 56.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 62.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 65.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 40/250 [00:00<00:03, 68.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  19% 48/250 [00:00<00:02, 70.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 56/250 [00:00<00:02, 68.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 63/250 [00:01<00:02, 63.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 60.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  31% 77/250 [00:01<00:02, 57.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  33% 83/250 [00:01<00:02, 56.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  36% 90/250 [00:01<00:02, 58.93it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 58.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 59.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  43% 108/250 [00:01<00:02, 58.22it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  46% 114/250 [00:01<00:02, 57.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 56.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 56.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  53% 132/250 [00:02<00:02, 56.24it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  55% 138/250 [00:02<00:01, 56.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 56.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 55.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  63% 157/250 [00:02<00:01, 57.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  65% 163/250 [00:02<00:01, 57.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  68% 169/250 [00:02<00:01, 58.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 176/250 [00:02<00:01, 59.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  73% 182/250 [00:03<00:01, 59.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  75% 188/250 [00:03<00:01, 56.98it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  78% 194/250 [00:03<00:00, 57.19it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 200/250 [00:03<00:00, 56.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  82% 206/250 [00:03<00:00, 53.73it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  85% 212/250 [00:03<00:00, 54.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  87% 218/250 [00:03<00:00, 54.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  90% 224/250 [00:03<00:00, 52.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 231/250 [00:03<00:00, 56.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 54.51it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 60.16it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 16:59:56 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.636 | nll_loss 0.005 | accuracy 80.4 | wps 61629.6 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-16 16:59:56 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-16 16:59:56 | INFO | train | epoch 005 | loss 0.498 | nll_loss 0.004 | accuracy 84.7 | wps 14459.6 | ups 13.55 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 6.80735e-06 | gnorm 9.817 | train_wall 11 | gb_free 14.3 | wall 87\n",
            "2023-11-16 16:59:56 | INFO | fairseq_cli.train | done training in 86.1 seconds\n",
            "2023-11-16 17:00:01.946705: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 17:00:01.946770: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 17:00:01.946815: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 17:00:01.958313: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 17:00:03.693620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 17:00:04 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-16 17:00:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 3125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2', 'restore_file': 'models/RoBERTa_small_fr/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=3125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2', restore_file='models/RoBERTa_small_fr/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=187, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='3125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 187, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 3125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-16 17:00:07 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-16 17:00:07 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-16 17:00:08 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-16 17:00:08 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-16 17:00:08 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-16 17:00:08 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-16 17:00:08 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-16 17:00:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-16 17:00:08 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/input0/valid\n",
            "2023-11-16 17:00:08 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/label/valid\n",
            "2023-11-16 17:00:08 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-16 17:00:08 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/input0/test\n",
            "2023-11-16 17:00:08 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/label/test\n",
            "2023-11-16 17:00:08 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-16 17:00:10 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-16 17:00:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:00:10 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-16 17:00:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:00:10 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-16 17:00:10 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-16 17:00:10 | INFO | fairseq.trainer | Preparing to load checkpoint models/RoBERTa_small_fr/model.pt\n",
            "2023-11-16 17:00:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.weight\n",
            "2023-11-16 17:00:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.dense.bias\n",
            "2023-11-16 17:00:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.weight\n",
            "2023-11-16 17:00:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.sentence_classification_head.out_proj.bias\n",
            "2023-11-16 17:00:11 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-16 17:00:11 | INFO | fairseq.trainer | Loaded checkpoint models/RoBERTa_small_fr/model.pt (epoch 10 @ 0 updates)\n",
            "2023-11-16 17:00:11 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-16 17:00:11 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/input0/train\n",
            "2023-11-16 17:00:11 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/label/train\n",
            "2023-11-16 17:00:11 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-16 17:00:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:00:11 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-16 17:00:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:13<00:00, 18.08it/s, loss=0.871, nll_loss=0.006, accuracy=82.5, wps=19050.4, ups=16.93, wpb=1125.2, bsz=8, num_updates=220, lr=9.88768e-06, gnorm=4.167, train_wall=0, gb_free=14.2, wall=14]2023-11-16 17:00:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:02,  8.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 24.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 43.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 20/25 [00:00<00:00, 56.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:00:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.931 | nll_loss 0.007 | accuracy 61.5 | wps 67370 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:00:25 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 17.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   3% 8/250 [00:00<00:06, 39.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 51.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 60.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 64.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 67.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 69.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 70.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 70.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 69.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 78/250 [00:01<00:02, 68.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 69.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 69.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 70.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  44% 110/250 [00:01<00:01, 71.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  47% 118/250 [00:01<00:01, 71.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 126/250 [00:01<00:01, 72.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  54% 134/250 [00:01<00:01, 72.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 71.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 70.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 71.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  66% 166/250 [00:02<00:01, 70.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  70% 174/250 [00:02<00:01, 72.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  73% 182/250 [00:02<00:00, 71.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 190/250 [00:02<00:00, 72.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  79% 198/250 [00:02<00:00, 70.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  82% 206/250 [00:03<00:00, 71.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 71.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 69.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 70.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  95% 238/250 [00:03<00:00, 72.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  98% 246/250 [00:03<00:00, 72.14it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:00:29 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.914 | nll_loss 0.007 | accuracy 63.1 | wps 73895.5 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:00:29 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-16 17:00:29 | INFO | train | epoch 001 | loss 0.983 | nll_loss 0.007 | accuracy 54.8 | wps 14218.3 | ups 13.3 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 9.87066e-06 | gnorm 3.436 | train_wall 12 | gb_free 14.3 | wall 18\n",
            "2023-11-16 17:00:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:00:29 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-16 17:00:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 223/225 [00:12<00:00, 18.12it/s, loss=0.669, nll_loss=0.006, accuracy=85, wps=15978, ups=19.22, wpb=831.4, bsz=8, num_updates=445, lr=9.12185e-06, gnorm=4.866, train_wall=0, gb_free=14.3, wall=30]2023-11-16 17:00:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 21.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 34.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 42.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 44.52it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:00:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.806 | nll_loss 0.006 | accuracy 70 | wps 51680.9 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 70\n",
            "2023-11-16 17:00:41 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:38,  6.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 35.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   6% 16/250 [00:00<00:05, 40.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 22/250 [00:00<00:05, 44.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  11% 28/250 [00:00<00:04, 48.16it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 49.63it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 50.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  18% 46/250 [00:01<00:03, 51.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 52.83it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 53.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 52.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 52.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  30% 76/250 [00:01<00:03, 52.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  33% 82/250 [00:01<00:03, 53.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  35% 88/250 [00:01<00:03, 53.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 53.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 100/250 [00:02<00:02, 53.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 106/250 [00:02<00:02, 53.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 112/250 [00:02<00:02, 54.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  47% 118/250 [00:02<00:02, 53.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  50% 124/250 [00:02<00:02, 54.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 130/250 [00:02<00:02, 55.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  54% 136/250 [00:02<00:02, 55.99it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 56.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  59% 148/250 [00:02<00:01, 56.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 154/250 [00:03<00:01, 55.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 160/250 [00:03<00:01, 51.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  66% 166/250 [00:03<00:01, 53.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  69% 172/250 [00:03<00:01, 53.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  71% 178/250 [00:03<00:01, 51.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 184/250 [00:03<00:01, 53.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  76% 190/250 [00:03<00:01, 55.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  78% 196/250 [00:03<00:00, 55.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  81% 202/250 [00:03<00:00, 56.53it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 209/250 [00:04<00:00, 57.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 215/250 [00:04<00:00, 56.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  88% 221/250 [00:04<00:00, 57.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  91% 228/250 [00:04<00:00, 60.86it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  94% 235/250 [00:04<00:00, 63.34it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  97% 243/250 [00:04<00:00, 67.47it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:00:46 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.749 | nll_loss 0.006 | accuracy 74.5 | wps 58159.9 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-16 17:00:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-16 17:00:46 | INFO | train | epoch 002 | loss 0.795 | nll_loss 0.006 | accuracy 72.2 | wps 13726.9 | ups 12.86 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 9.10483e-06 | gnorm 5.492 | train_wall 11 | gb_free 14.3 | wall 36\n",
            "2023-11-16 17:00:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:00:46 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-16 17:00:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 224/225 [00:11<00:00, 18.47it/s, loss=0.536, nll_loss=0.004, accuracy=87.5, wps=21802.9, ups=18.45, wpb=1182, bsz=8, num_updates=670, lr=8.35602e-06, gnorm=7.399, train_wall=0, gb_free=14.3, wall=47]2023-11-16 17:00:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 21.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 36.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 39.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 45.90it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:00:58 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.595 | nll_loss 0.005 | accuracy 81.5 | wps 54268.5 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 81.5\n",
            "2023-11-16 17:00:58 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:33,  7.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   2% 6/250 [00:00<00:08, 29.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   5% 12/250 [00:00<00:05, 40.28it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   7% 18/250 [00:00<00:05, 46.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 48.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 48.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 51.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 41/250 [00:00<00:03, 53.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  19% 47/250 [00:00<00:03, 52.59it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 55.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 53.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  26% 66/250 [00:01<00:03, 55.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 54.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 55.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  34% 84/250 [00:01<00:03, 54.33it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  36% 90/250 [00:01<00:02, 53.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 55.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 54.44it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  43% 108/250 [00:02<00:02, 54.82it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 54.76it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 54.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 55.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  53% 132/250 [00:02<00:02, 54.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  55% 138/250 [00:02<00:02, 52.43it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 53.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 52.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  62% 156/250 [00:03<00:01, 51.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  65% 162/250 [00:03<00:01, 52.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  67% 168/250 [00:03<00:01, 52.30it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  70% 174/250 [00:03<00:01, 50.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  72% 180/250 [00:03<00:01, 51.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  75% 187/250 [00:03<00:01, 53.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  77% 193/250 [00:03<00:01, 53.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  80% 199/250 [00:03<00:00, 53.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  82% 205/250 [00:03<00:00, 53.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  84% 211/250 [00:04<00:00, 53.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  87% 217/250 [00:04<00:00, 54.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  89% 223/250 [00:04<00:00, 54.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 229/250 [00:04<00:00, 54.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  94% 235/250 [00:04<00:00, 54.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  96% 241/250 [00:04<00:00, 55.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  99% 248/250 [00:04<00:00, 57.16it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:01:03 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.643 | nll_loss 0.005 | accuracy 79.4 | wps 56574.8 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-16 17:01:03 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-16 17:01:03 | INFO | train | epoch 003 | loss 0.651 | nll_loss 0.005 | accuracy 79.7 | wps 13959 | ups 13.08 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 8.33901e-06 | gnorm 7.206 | train_wall 11 | gb_free 14.2 | wall 53\n",
            "2023-11-16 17:01:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:01:03 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-16 17:01:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 222/225 [00:11<00:00, 20.82it/s, loss=0.704, nll_loss=0.005, accuracy=80, wps=23691, ups=20.49, wpb=1156, bsz=8, num_updates=895, lr=7.5902e-06, gnorm=8.683, train_wall=0, gb_free=14.3, wall=65]       2023-11-16 17:01:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 16.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 43.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 56.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:01:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.555 | nll_loss 0.004 | accuracy 84 | wps 73657.3 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 84\n",
            "2023-11-16 17:01:16 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:12, 19.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   3% 8/250 [00:00<00:06, 40.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 52.79it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 61.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  12% 30/250 [00:00<00:03, 64.32it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  15% 37/250 [00:00<00:03, 60.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  18% 44/250 [00:00<00:03, 55.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  20% 50/250 [00:00<00:03, 56.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 56/250 [00:01<00:03, 56.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  25% 62/250 [00:01<00:03, 55.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  27% 68/250 [00:01<00:03, 56.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  30% 74/250 [00:01<00:03, 56.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 56.37it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 55.77it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 56.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  39% 98/250 [00:01<00:02, 56.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  42% 104/250 [00:01<00:02, 56.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 110/250 [00:01<00:02, 56.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  46% 116/250 [00:02<00:02, 56.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  49% 122/250 [00:02<00:02, 57.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 58.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:02<00:01, 58.28it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  56% 140/250 [00:02<00:01, 58.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  58% 146/250 [00:02<00:01, 58.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 57.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 57.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 164/250 [00:02<00:01, 55.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  68% 171/250 [00:03<00:01, 58.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  71% 177/250 [00:03<00:01, 57.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 183/250 [00:03<00:01, 55.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 189/250 [00:03<00:01, 56.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  78% 195/250 [00:03<00:00, 55.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 55.17it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 56.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 56.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 57.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 58.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  93% 233/250 [00:04<00:00, 58.27it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  96% 239/250 [00:04<00:00, 58.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 55.57it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:01:20 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.648 | nll_loss 0.005 | accuracy 80.1 | wps 60249.4 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-16 17:01:20 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-16 17:01:20 | INFO | train | epoch 004 | loss 0.572 | nll_loss 0.004 | accuracy 82.3 | wps 14367.8 | ups 13.46 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 7.57318e-06 | gnorm 8.371 | train_wall 11 | gb_free 14.3 | wall 70\n",
            "2023-11-16 17:01:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:01:20 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-16 17:01:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 224/225 [00:12<00:00, 21.47it/s, loss=0.621, nll_loss=0.005, accuracy=77.5, wps=23062.3, ups=21.07, wpb=1094.4, bsz=8, num_updates=1120, lr=6.82437e-06, gnorm=11.123, train_wall=0, gb_free=14.3, wall=82]2023-11-16 17:01:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 16.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 43.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 57.20it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:01:33 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.652 | nll_loss 0.005 | accuracy 77.5 | wps 75149.7 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 84\n",
            "2023-11-16 17:01:33 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:24,  9.97it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 42.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 53.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   9% 22/250 [00:00<00:03, 59.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  12% 30/250 [00:00<00:03, 65.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 67.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  18% 46/250 [00:00<00:02, 68.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 54/250 [00:00<00:02, 70.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 62/250 [00:00<00:02, 70.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 72.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  31% 78/250 [00:01<00:02, 73.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 69.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 69.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 70.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  44% 110/250 [00:01<00:02, 69.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  47% 118/250 [00:01<00:01, 70.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 126/250 [00:01<00:01, 72.01it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  54% 134/250 [00:01<00:01, 72.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 72.24it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 72.13it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 69.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  66% 166/250 [00:02<00:01, 70.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 174/250 [00:02<00:01, 71.30it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  73% 182/250 [00:02<00:00, 72.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 190/250 [00:02<00:00, 72.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  79% 198/250 [00:02<00:00, 71.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  82% 206/250 [00:02<00:00, 72.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 71.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 72.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 72.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  95% 238/250 [00:03<00:00, 71.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 246/250 [00:03<00:00, 73.50it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:01:36 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.658 | nll_loss 0.005 | accuracy 80.4 | wps 74685 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-16 17:01:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-16 17:01:36 | INFO | train | epoch 005 | loss 0.506 | nll_loss 0.004 | accuracy 85.2 | wps 14563.9 | ups 13.65 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 6.80735e-06 | gnorm 8.743 | train_wall 11 | gb_free 14.3 | wall 86\n",
            "2023-11-16 17:01:36 | INFO | fairseq_cli.train | done training in 85.6 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Random $RoBERTa_{small}^{fr}$ model training:</b>\n",
        "\n",
        "In this section you have to finetune a random checkpinf of the model $RoBERTa_{small}^{fr}$ using the same setting as before (<b>Hint:</b> an unexisted model path will not give you an error)"
      ],
      "metadata": {
        "id": "wi1U19Uunnse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "TASK='sentence_prediction' # fill me, sentence prediction task on fairseq\n",
        "MODEL='RoBERTa_small_fr_random'\n",
        "DATA_PATH='data/cls-books-bin' # fill me\n",
        "MODEL_PATH='random_model' # fill me\n",
        "MAX_EPOCH=5 # fill me\n",
        "MAX_SENTENCES=8 # fill me, batch size\n",
        "MAX_UPDATE=3125 # fill me, n_epochs * n_train_examples / total batch size\n",
        "LR=1e-5 # fill me\n",
        "VALID_SUBSET='valid,test' # for simplicity we will validate on both valid and test set, and then pick the value of test set corresponding the best validation score.\n",
        "METRIC='accuracy' # fill me, use the accuracy metric\n",
        "NUM_CLASSES=2 #fill me, number of classes\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0\n",
        "WARMUP=187 # fill me, warmup ratio=6% of the whole training"
      ],
      "metadata": {
        "id": "lSLWP6VUhUlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "  TENSORBOARD_LOGS= 'tensorboard_logs/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  SAVE_DIR= 'checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_mu'+str(MAX_UPDATE)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "  !(python libs/fairseq/fairseq_cli/train.py $DATA_PATH \\\n",
        "                --restore-file $MODEL_PATH \\\n",
        "                --batch-size $MAX_SENTENCES \\\n",
        "                --task $TASK \\\n",
        "                --update-freq 1 \\\n",
        "                --seed $SEED \\\n",
        "                --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "                --init-token 0 \\\n",
        "                --separator-token 2 \\\n",
        "                --arch roberta_small \\\n",
        "                --criterion sentence_prediction \\\n",
        "                --num-classes $NUM_CLASSES \\\n",
        "                --weight-decay 0.01 \\\n",
        "                --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-08 \\\n",
        "                --maximize-best-checkpoint-metric \\\n",
        "                --best-checkpoint-metric 'accuracy' \\\n",
        "                --save-dir $SAVE_DIR \\\n",
        "                --lr-scheduler polynomial_decay \\\n",
        "                --lr $LR \\\n",
        "                --max-update $MAX_UPDATE \\\n",
        "                --total-num-update $MAX_UPDATE \\\n",
        "                --no-epoch-checkpoints \\\n",
        "                --no-last-checkpoints \\\n",
        "                --tensorboard-logdir $TENSORBOARD_LOGS \\\n",
        "                --log-interval 5 \\\n",
        "                --warmup-updates $((6*$MAX_UPDATE/100)) \\\n",
        "                --max-epoch $MAX_EPOCH \\\n",
        "                --keep-best-checkpoints 1 \\\n",
        "                --max-positions 256 \\\n",
        "                --valid-subset $VALID_SUBSET \\\n",
        "                --shorten-method 'truncate' \\\n",
        "                --no-save \\\n",
        "                --distributed-world-size 1)"
      ],
      "metadata": {
        "id": "qtsCysc4hb42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e4eeb8-8d2d-46b6-a8dd-c3f06fedac37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 17:01:41.808000: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 17:01:41.808054: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 17:01:41.808093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 17:01:41.819607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 17:01:43.535303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 17:01:45 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-16 17:01:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0', 'wandb_project': None, 'azureml_logging': False, 'seed': 0, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 3125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0', 'restore_file': 'random_model', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0', wandb_project=None, azureml_logging=False, seed=0, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=3125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0', restore_file='random_model', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=187, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='3125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 0}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 187, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 3125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-16 17:01:48 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-16 17:01:48 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-16 17:01:49 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-16 17:01:49 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-16 17:01:49 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-16 17:01:49 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-16 17:01:49 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-16 17:01:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-16 17:01:49 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/input0/valid\n",
            "2023-11-16 17:01:49 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/label/valid\n",
            "2023-11-16 17:01:49 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-16 17:01:49 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/input0/test\n",
            "2023-11-16 17:01:49 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/label/test\n",
            "2023-11-16 17:01:49 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-16 17:01:51 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-16 17:01:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:01:51 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-16 17:01:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:01:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-16 17:01:51 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-16 17:01:51 | INFO | fairseq.trainer | Preparing to load checkpoint random_model\n",
            "2023-11-16 17:01:51 | INFO | fairseq.trainer | No existing checkpoint found random_model\n",
            "2023-11-16 17:01:51 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-16 17:01:51 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/input0/train\n",
            "2023-11-16 17:01:51 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/label/train\n",
            "2023-11-16 17:01:51 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-16 17:01:51 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-16 17:01:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:01:52 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-16 17:01:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:12<00:00, 17.81it/s, loss=0.944, nll_loss=0.007, accuracy=75, wps=18865.2, ups=17.59, wpb=1072.6, bsz=8, num_updates=220, lr=9.88768e-06, gnorm=4.286, train_wall=0, gb_free=14.2, wall=13]2023-11-16 17:02:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 23.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 36.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 39.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 44.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:02:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.007 | nll_loss 0.008 | accuracy 50 | wps 51006.3 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:02:05 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   0% 1/250 [00:00<00:37,  6.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   2% 6/250 [00:00<00:09, 26.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   5% 12/250 [00:00<00:06, 38.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 18/250 [00:00<00:05, 44.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 48.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 50.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 36/250 [00:00<00:04, 51.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  17% 42/250 [00:00<00:04, 51.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 48/250 [00:01<00:03, 52.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 52.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 52.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 66/250 [00:01<00:03, 52.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 52.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 52.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 84/250 [00:01<00:03, 52.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 90/250 [00:01<00:02, 53.47it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 54.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  41% 102/250 [00:02<00:02, 54.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 108/250 [00:02<00:02, 55.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 54.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  48% 121/250 [00:02<00:02, 58.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 60.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  54% 135/250 [00:02<00:01, 63.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 65.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 151/250 [00:02<00:01, 67.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 68.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 69.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  70% 174/250 [00:03<00:01, 69.19it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  73% 182/250 [00:03<00:00, 69.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 190/250 [00:03<00:00, 70.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 70.29it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  82% 206/250 [00:03<00:00, 69.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 70.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 70.93it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 70.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  95% 238/250 [00:04<00:00, 71.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 71.45it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:02:09 | INFO | test | epoch 001 | valid on 'test' subset | loss 1.004 | nll_loss 0.008 | accuracy 50 | wps 64301.6 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:02:09 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-16 17:02:09 | INFO | train | epoch 001 | loss 1.004 | nll_loss 0.008 | accuracy 51.4 | wps 14201.3 | ups 13.3 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 9.87066e-06 | gnorm 4.606 | train_wall 12 | gb_free 14.3 | wall 18\n",
            "2023-11-16 17:02:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:02:09 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-16 17:02:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 223/225 [00:11<00:00, 18.61it/s, loss=1.058, nll_loss=0.007, accuracy=35, wps=21908.6, ups=18.53, wpb=1182.2, bsz=8, num_updates=445, lr=9.12185e-06, gnorm=6.832, train_wall=0, gb_free=14.3, wall=30]2023-11-16 17:02:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 23.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 37.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 18/25 [00:00<00:00, 48.23it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 50.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:02:22 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.988 | nll_loss 0.008 | accuracy 54 | wps 55810.9 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 54\n",
            "2023-11-16 17:02:22 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:35,  7.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 36.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 43.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 46.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 49.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 50.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  16% 41/250 [00:00<00:04, 50.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  19% 47/250 [00:01<00:03, 51.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  21% 53/250 [00:01<00:03, 50.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 51.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 51.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 50.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 50.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 50.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 89/250 [00:01<00:03, 51.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 95/250 [00:01<00:03, 51.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  40% 101/250 [00:02<00:02, 51.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 51.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  45% 113/250 [00:02<00:02, 51.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 51.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 51.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 51.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 51.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  57% 143/250 [00:02<00:02, 51.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  60% 149/250 [00:03<00:01, 50.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 52.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 53.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 53.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 54.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 55.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 55.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 54.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  79% 197/250 [00:03<00:00, 54.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  81% 203/250 [00:04<00:00, 54.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 209/250 [00:04<00:00, 54.81it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  86% 215/250 [00:04<00:00, 54.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  88% 221/250 [00:04<00:00, 54.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  91% 227/250 [00:04<00:00, 53.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  93% 233/250 [00:04<00:00, 53.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 240/250 [00:04<00:00, 52.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 51.60it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:02:27 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.976 | nll_loss 0.007 | accuracy 55.8 | wps 54904 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-16 17:02:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-16 17:02:27 | INFO | train | epoch 002 | loss 0.995 | nll_loss 0.007 | accuracy 53.9 | wps 13816.6 | ups 12.95 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 9.10483e-06 | gnorm 4.581 | train_wall 11 | gb_free 14.3 | wall 35\n",
            "2023-11-16 17:02:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:02:27 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-16 17:02:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 223/225 [00:11<00:00, 18.88it/s, loss=0.947, nll_loss=0.007, accuracy=67.5, wps=20917, ups=19.07, wpb=1096.6, bsz=8, num_updates=670, lr=8.35602e-06, gnorm=2.985, train_wall=0, gb_free=14.3, wall=47]2023-11-16 17:02:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 23.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 37.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 40.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 45.24it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:02:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.948 | nll_loss 0.007 | accuracy 62.5 | wps 53339.7 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 62.5\n",
            "2023-11-16 17:02:39 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   0% 1/250 [00:00<00:37,  6.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   3% 7/250 [00:00<00:08, 29.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 14/250 [00:00<00:05, 43.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   8% 20/250 [00:00<00:04, 48.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  11% 27/250 [00:00<00:04, 53.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  13% 33/250 [00:00<00:04, 54.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 54.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  18% 45/250 [00:00<00:03, 54.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  20% 51/250 [00:01<00:03, 54.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  23% 57/250 [00:01<00:03, 55.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  25% 63/250 [00:01<00:03, 55.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  28% 69/250 [00:01<00:03, 56.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 75/250 [00:01<00:03, 56.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 56.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 55.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  37% 93/250 [00:01<00:02, 56.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  40% 99/250 [00:01<00:02, 56.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  42% 105/250 [00:01<00:02, 56.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 111/250 [00:02<00:02, 56.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 117/250 [00:02<00:02, 55.57it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  49% 123/250 [00:02<00:02, 55.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  52% 129/250 [00:02<00:02, 55.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 135/250 [00:02<00:02, 55.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 56.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 56.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 56.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 54.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  66% 165/250 [00:03<00:01, 53.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  68% 171/250 [00:03<00:01, 54.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  71% 177/250 [00:03<00:01, 54.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  73% 183/250 [00:03<00:01, 54.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  76% 189/250 [00:03<00:01, 52.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  78% 195/250 [00:03<00:01, 54.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  80% 201/250 [00:03<00:00, 54.55it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 54.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 53.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 219/250 [00:04<00:00, 53.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  90% 225/250 [00:04<00:00, 54.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 231/250 [00:04<00:00, 54.17it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 54.42it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 59.79it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:02:44 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.921 | nll_loss 0.007 | accuracy 67.1 | wps 58607.1 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-16 17:02:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-16 17:02:44 | INFO | train | epoch 003 | loss 0.952 | nll_loss 0.007 | accuracy 61.9 | wps 13986.7 | ups 13.11 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 8.33901e-06 | gnorm 4.675 | train_wall 11 | gb_free 14.3 | wall 52\n",
            "2023-11-16 17:02:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:02:44 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-16 17:02:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 21.04it/s, loss=0.752, nll_loss=0.005, accuracy=75, wps=23768, ups=20.53, wpb=1157.6, bsz=8, num_updates=895, lr=7.5902e-06, gnorm=5.297, train_wall=0, gb_free=14.3, wall=64]2023-11-16 17:02:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 17.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 45.03it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 59.61it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:02:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.927 | nll_loss 0.007 | accuracy 62 | wps 75977.8 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 62.5\n",
            "2023-11-16 17:02:57 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:13, 18.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 43.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   7% 17/250 [00:00<00:04, 57.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 63.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  13% 33/250 [00:00<00:03, 67.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 41/250 [00:00<00:02, 70.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  20% 49/250 [00:00<00:02, 69.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  23% 57/250 [00:00<00:02, 70.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  26% 65/250 [00:00<00:02, 71.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  29% 73/250 [00:01<00:02, 70.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 81/250 [00:01<00:02, 69.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  36% 89/250 [00:01<00:02, 70.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  39% 97/250 [00:01<00:02, 70.68it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  42% 105/250 [00:01<00:02, 71.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  45% 113/250 [00:01<00:01, 71.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  48% 121/250 [00:01<00:01, 71.55it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  52% 129/250 [00:01<00:01, 70.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  55% 137/250 [00:02<00:01, 71.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  58% 145/250 [00:02<00:01, 70.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 71.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  64% 161/250 [00:02<00:01, 68.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  67% 168/250 [00:02<00:01, 64.71it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  70% 175/250 [00:02<00:01, 62.88it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 182/250 [00:02<00:01, 60.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 189/250 [00:02<00:01, 59.82it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  78% 196/250 [00:02<00:00, 59.36it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  81% 202/250 [00:03<00:00, 58.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  83% 208/250 [00:03<00:00, 57.89it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 57.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  88% 220/250 [00:03<00:00, 58.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  90% 226/250 [00:03<00:00, 58.21it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  93% 232/250 [00:03<00:00, 57.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  96% 239/250 [00:03<00:00, 59.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 245/250 [00:03<00:00, 53.84it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:03:00 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.871 | nll_loss 0.007 | accuracy 65.5 | wps 68140 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-16 17:03:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-16 17:03:00 | INFO | train | epoch 004 | loss 0.866 | nll_loss 0.006 | accuracy 68.9 | wps 14467.8 | ups 13.56 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 7.57318e-06 | gnorm 5.069 | train_wall 11 | gb_free 14.3 | wall 69\n",
            "2023-11-16 17:03:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:03:00 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-16 17:03:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:12<00:00, 20.85it/s, loss=0.691, nll_loss=0.005, accuracy=77.5, wps=23554.7, ups=21.09, wpb=1117, bsz=8, num_updates=1120, lr=6.82437e-06, gnorm=8.513, train_wall=0, gb_free=14.3, wall=81]2023-11-16 17:03:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 17.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 38.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 55.60it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 66.93it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:03:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.839 | nll_loss 0.006 | accuracy 67 | wps 73686 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 67\n",
            "2023-11-16 17:03:13 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 17.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   3% 8/250 [00:00<00:05, 40.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 54.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  10% 24/250 [00:00<00:03, 63.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 68.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 40/250 [00:00<00:02, 70.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  19% 48/250 [00:00<00:02, 69.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 68.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 70.47it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 71.61it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  32% 79/250 [00:01<00:02, 71.76it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 72.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 71.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 71.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  44% 111/250 [00:01<00:01, 71.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 119/250 [00:01<00:01, 70.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  51% 127/250 [00:01<00:01, 70.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  54% 135/250 [00:01<00:01, 70.69it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  57% 143/250 [00:02<00:01, 70.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 151/250 [00:02<00:01, 71.33it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  64% 159/250 [00:02<00:01, 71.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  67% 167/250 [00:02<00:01, 72.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 175/250 [00:02<00:01, 72.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  73% 183/250 [00:02<00:00, 71.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 191/250 [00:02<00:00, 71.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 199/250 [00:02<00:00, 70.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  83% 207/250 [00:02<00:00, 70.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 70.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 69.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 68.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  95% 238/250 [00:03<00:00, 70.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 246/250 [00:03<00:00, 70.62it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:03:17 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.796 | nll_loss 0.006 | accuracy 73.9 | wps 74255.4 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-16 17:03:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-16 17:03:17 | INFO | train | epoch 005 | loss 0.738 | nll_loss 0.006 | accuracy 76.2 | wps 14438.7 | ups 13.53 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 6.80735e-06 | gnorm 5.913 | train_wall 11 | gb_free 14.3 | wall 86\n",
            "2023-11-16 17:03:17 | INFO | fairseq_cli.train | done training in 85.4 seconds\n",
            "2023-11-16 17:03:21.653867: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 17:03:21.653927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 17:03:21.653969: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 17:03:21.666599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 17:03:23.303226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 17:03:24 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-16 17:03:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 3125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1', 'restore_file': 'random_model', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=3125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1', restore_file='random_model', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=187, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='3125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 187, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 3125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-16 17:03:29 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-16 17:03:29 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-16 17:03:30 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-16 17:03:30 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-16 17:03:30 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-16 17:03:30 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-16 17:03:30 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-16 17:03:30 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-16 17:03:30 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/input0/valid\n",
            "2023-11-16 17:03:30 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/label/valid\n",
            "2023-11-16 17:03:30 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-16 17:03:30 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/input0/test\n",
            "2023-11-16 17:03:30 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/label/test\n",
            "2023-11-16 17:03:30 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-16 17:03:32 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-16 17:03:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:03:32 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-16 17:03:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:03:32 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-16 17:03:32 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-16 17:03:32 | INFO | fairseq.trainer | Preparing to load checkpoint random_model\n",
            "2023-11-16 17:03:32 | INFO | fairseq.trainer | No existing checkpoint found random_model\n",
            "2023-11-16 17:03:32 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-16 17:03:32 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/input0/train\n",
            "2023-11-16 17:03:32 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/label/train\n",
            "2023-11-16 17:03:32 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-16 17:03:32 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-16 17:03:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:03:33 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-16 17:03:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 224/225 [00:12<00:00, 17.80it/s, loss=1.001, nll_loss=0.008, accuracy=50, wps=18368.4, ups=17.71, wpb=1037, bsz=8, num_updates=220, lr=9.88768e-06, gnorm=4.633, train_wall=0, gb_free=14.3, wall=13]2023-11-16 17:03:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 22.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 36.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 40.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 22/25 [00:00<00:00, 46.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:03:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.997 | nll_loss 0.008 | accuracy 50.5 | wps 54221.7 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:03:46 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   0% 1/250 [00:00<00:35,  6.95it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   2% 6/250 [00:00<00:09, 26.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   5% 12/250 [00:00<00:06, 37.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 18/250 [00:00<00:05, 44.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 47.70it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 50.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 36/250 [00:00<00:04, 51.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  17% 42/250 [00:00<00:03, 53.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 48/250 [00:01<00:04, 49.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  21% 53/250 [00:01<00:04, 48.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 59/250 [00:01<00:03, 50.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 65/250 [00:01<00:03, 51.89it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  28% 71/250 [00:01<00:03, 52.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 77/250 [00:01<00:03, 52.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  33% 83/250 [00:01<00:03, 52.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 89/250 [00:01<00:03, 53.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 54.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  40% 101/250 [00:02<00:02, 54.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 107/250 [00:02<00:02, 53.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 55.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 54.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 52.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  53% 132/250 [00:02<00:02, 52.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  55% 138/250 [00:02<00:02, 53.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  58% 144/250 [00:02<00:02, 52.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 54.77it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  63% 157/250 [00:03<00:01, 57.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  66% 164/250 [00:03<00:01, 59.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  68% 170/250 [00:03<00:01, 59.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  70% 176/250 [00:03<00:01, 59.50it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  73% 182/250 [00:03<00:01, 58.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 189/250 [00:03<00:01, 60.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  78% 196/250 [00:03<00:00, 58.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  81% 202/250 [00:03<00:00, 57.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  83% 208/250 [00:03<00:00, 57.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  86% 214/250 [00:04<00:00, 57.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  88% 221/250 [00:04<00:00, 58.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  92% 229/250 [00:04<00:00, 63.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  95% 237/250 [00:04<00:00, 67.08it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  98% 245/250 [00:04<00:00, 70.16it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:03:50 | INFO | test | epoch 001 | valid on 'test' subset | loss 0.993 | nll_loss 0.008 | accuracy 51.5 | wps 59378.9 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:03:50 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-16 17:03:50 | INFO | train | epoch 001 | loss 1.006 | nll_loss 0.008 | accuracy 49 | wps 14048.7 | ups 13.18 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 9.87066e-06 | gnorm 4.569 | train_wall 12 | gb_free 14.3 | wall 18\n",
            "2023-11-16 17:03:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:03:50 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-16 17:03:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 223/225 [00:11<00:00, 18.40it/s, loss=0.934, nll_loss=0.007, accuracy=67.5, wps=20100.3, ups=19.76, wpb=1017.4, bsz=8, num_updates=445, lr=9.12185e-06, gnorm=5.151, train_wall=0, gb_free=14.2, wall=29]2023-11-16 17:04:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 27.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 39.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 42.44it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 49.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:04:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.978 | nll_loss 0.007 | accuracy 60.5 | wps 56353.9 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 60.5\n",
            "2023-11-16 17:04:03 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   0% 1/250 [00:00<00:32,  7.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 37.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 43.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 47.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 51.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  14% 35/250 [00:00<00:04, 51.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  17% 42/250 [00:00<00:03, 54.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  19% 48/250 [00:01<00:03, 55.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 55.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 56.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  26% 66/250 [00:01<00:03, 56.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 56.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 55.74it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  34% 84/250 [00:01<00:03, 54.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  36% 90/250 [00:01<00:02, 54.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 54.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 54.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  43% 108/250 [00:02<00:02, 54.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 54.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 54.66it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 54.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  53% 132/250 [00:02<00:02, 53.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  55% 138/250 [00:02<00:02, 52.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 53.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 54.06it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  62% 156/250 [00:02<00:01, 54.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  65% 162/250 [00:03<00:01, 55.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  67% 168/250 [00:03<00:01, 54.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  70% 174/250 [00:03<00:01, 55.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  72% 180/250 [00:03<00:01, 54.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  74% 186/250 [00:03<00:01, 54.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  77% 192/250 [00:03<00:01, 55.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  80% 199/250 [00:03<00:00, 56.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  82% 205/250 [00:03<00:00, 56.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  84% 211/250 [00:03<00:00, 53.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  87% 217/250 [00:04<00:00, 54.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  89% 223/250 [00:04<00:00, 52.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  92% 229/250 [00:04<00:00, 52.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  94% 235/250 [00:04<00:00, 53.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  96% 241/250 [00:04<00:00, 50.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  99% 247/250 [00:04<00:00, 53.04it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:04:07 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.967 | nll_loss 0.007 | accuracy 62.8 | wps 56430.7 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-16 17:04:07 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-16 17:04:07 | INFO | train | epoch 002 | loss 0.988 | nll_loss 0.007 | accuracy 55.9 | wps 14173.3 | ups 13.28 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 9.10483e-06 | gnorm 4.635 | train_wall 11 | gb_free 14.3 | wall 35\n",
            "2023-11-16 17:04:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:04:07 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-16 17:04:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 222/225 [00:12<00:00, 20.00it/s, loss=0.869, nll_loss=0.006, accuracy=67.5, wps=22604.1, ups=20.3, wpb=1113.4, bsz=8, num_updates=670, lr=8.35602e-06, gnorm=4.806, train_wall=0, gb_free=14.3, wall=47]2023-11-16 17:04:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.07it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  28% 7/25 [00:00<00:00, 35.91it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 15/25 [00:00<00:00, 54.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 65.58it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:04:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.933 | nll_loss 0.007 | accuracy 67.5 | wps 71781.9 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 67.5\n",
            "2023-11-16 17:04:20 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 16.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   3% 8/250 [00:00<00:06, 38.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 54.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 60.04it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 64.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 67.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  18% 46/250 [00:00<00:03, 64.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  21% 53/250 [00:00<00:03, 64.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  24% 60/250 [00:00<00:03, 63.01it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  27% 67/250 [00:01<00:03, 59.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  30% 74/250 [00:01<00:03, 57.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 56.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 54.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 55.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  39% 98/250 [00:01<00:02, 56.92it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  42% 104/250 [00:01<00:02, 55.29it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 111/250 [00:01<00:02, 56.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 117/250 [00:02<00:02, 55.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  49% 123/250 [00:02<00:02, 55.93it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  52% 129/250 [00:02<00:02, 56.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  54% 135/250 [00:02<00:02, 54.44it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 55.25it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  59% 147/250 [00:02<00:01, 56.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  61% 153/250 [00:02<00:01, 56.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  64% 160/250 [00:02<00:01, 57.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  67% 167/250 [00:02<00:01, 58.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 57.67it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 57.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 56.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 56.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  79% 197/250 [00:03<00:00, 56.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  81% 203/250 [00:03<00:00, 55.54it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  84% 209/250 [00:03<00:00, 56.16it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 55.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 221/250 [00:03<00:00, 56.72it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  91% 227/250 [00:03<00:00, 56.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  93% 233/250 [00:04<00:00, 57.26it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  96% 240/250 [00:04<00:00, 57.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 246/250 [00:04<00:00, 53.72it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:04:24 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.906 | nll_loss 0.007 | accuracy 69.2 | wps 60518.5 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-16 17:04:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-16 17:04:24 | INFO | train | epoch 003 | loss 0.939 | nll_loss 0.007 | accuracy 63.8 | wps 14013.2 | ups 13.13 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 8.33901e-06 | gnorm 4.726 | train_wall 11 | gb_free 14.3 | wall 52\n",
            "2023-11-16 17:04:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:04:24 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-16 17:04:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 21.29it/s, loss=0.723, nll_loss=0.006, accuracy=82.5, wps=23491.5, ups=23.47, wpb=1001, bsz=8, num_updates=895, lr=7.5902e-06, gnorm=3.44, train_wall=0, gb_free=14.3, wall=64]2023-11-16 17:04:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 17.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  36% 9/25 [00:00<00:00, 44.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 58.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 65.55it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:04:37 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.87 | nll_loss 0.007 | accuracy 65.5 | wps 72377.1 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 67.5\n",
            "2023-11-16 17:04:37 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 17.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 44.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   7% 17/250 [00:00<00:04, 56.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  10% 25/250 [00:00<00:03, 63.25it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  13% 32/250 [00:00<00:03, 64.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 65.85it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 69.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 70.38it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 71.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 71.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  32% 79/250 [00:01<00:02, 71.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 70.43it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 70.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  41% 103/250 [00:01<00:02, 69.61it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 110/250 [00:01<00:02, 68.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  47% 118/250 [00:01<00:01, 70.19it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  50% 126/250 [00:01<00:01, 71.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:01<00:01, 70.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 71.43it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 71.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 71.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 166/250 [00:02<00:01, 72.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  70% 174/250 [00:02<00:01, 72.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 182/250 [00:02<00:00, 72.07it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 190/250 [00:02<00:00, 71.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  79% 198/250 [00:02<00:00, 70.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  82% 206/250 [00:02<00:00, 71.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 70.92it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 70.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 71.63it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  95% 238/250 [00:03<00:00, 72.98it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 246/250 [00:03<00:00, 73.57it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:04:41 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.826 | nll_loss 0.006 | accuracy 71.5 | wps 74470.1 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-16 17:04:41 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-16 17:04:41 | INFO | train | epoch 004 | loss 0.841 | nll_loss 0.006 | accuracy 71.1 | wps 14646.2 | ups 13.72 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 7.57318e-06 | gnorm 5.297 | train_wall 11 | gb_free 14.3 | wall 69\n",
            "2023-11-16 17:04:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:04:41 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-16 17:04:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:12<00:00, 20.55it/s, loss=0.553, nll_loss=0.005, accuracy=77.5, wps=20625.6, ups=21.63, wpb=953.4, bsz=8, num_updates=1120, lr=6.82437e-06, gnorm=6.557, train_wall=0, gb_free=14.3, wall=81]2023-11-16 17:04:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 39.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 56.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 67.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:04:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.835 | nll_loss 0.006 | accuracy 70 | wps 73398.4 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 70\n",
            "2023-11-16 17:04:54 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:25,  9.94it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   3% 7/250 [00:00<00:06, 37.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 55.44it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   9% 22/250 [00:00<00:03, 57.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  12% 30/250 [00:00<00:03, 63.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 66.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  18% 46/250 [00:00<00:02, 68.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  22% 54/250 [00:00<00:02, 69.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  24% 61/250 [00:00<00:02, 69.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 69/250 [00:01<00:02, 71.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  31% 77/250 [00:01<00:02, 71.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  34% 85/250 [00:01<00:02, 71.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  37% 93/250 [00:01<00:02, 73.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  40% 101/250 [00:01<00:02, 71.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  44% 109/250 [00:01<00:01, 72.28it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  47% 117/250 [00:01<00:01, 71.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 125/250 [00:01<00:01, 70.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  53% 133/250 [00:01<00:01, 70.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 71.05it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 71.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  63% 157/250 [00:02<00:01, 71.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  66% 165/250 [00:02<00:01, 73.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  69% 173/250 [00:02<00:01, 70.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  72% 181/250 [00:02<00:00, 72.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  76% 189/250 [00:02<00:00, 72.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  79% 197/250 [00:02<00:00, 71.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  82% 205/250 [00:02<00:00, 71.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 72.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  88% 221/250 [00:03<00:00, 71.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 229/250 [00:03<00:00, 73.16it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  95% 237/250 [00:03<00:00, 72.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  98% 245/250 [00:03<00:00, 71.41it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:04:57 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.785 | nll_loss 0.006 | accuracy 73.9 | wps 74827.7 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-16 17:04:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-16 17:04:57 | INFO | train | epoch 005 | loss 0.73 | nll_loss 0.005 | accuracy 75.3 | wps 14417.3 | ups 13.51 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 6.80735e-06 | gnorm 6.367 | train_wall 11 | gb_free 14.3 | wall 85\n",
            "2023-11-16 17:04:58 | INFO | fairseq_cli.train | done training in 84.9 seconds\n",
            "2023-11-16 17:05:01.455964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 17:05:01.456025: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 17:05:01.456066: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 17:05:01.463812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 17:05:02.594324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-16 17:05:04 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-16 17:05:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2', 'wandb_project': None, 'azureml_logging': False, 'seed': 2, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid,test', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 3125, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2', 'restore_file': 'random_model', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=5, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2', wandb_project=None, azureml_logging=False, seed=2, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=None, batch_size=8, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid,test', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=None, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_small', max_epoch=5, max_update=3125, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2', restore_file='random_model', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=1, no_save=True, no_epoch_checkpoints=True, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='data/cls-books-bin', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='sentence_classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.01, use_old_adam=False, fp16_adam_stats=False, warmup_updates=187, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='3125', pad=1, eos=2, unk=3, max_positions=256, no_seed_provided=False, encoder_layers=4, encoder_embed_dim=512, encoder_ffn_embed_dim=512, encoder_attention_heads=8, max_source_positions=256, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_small'), 'task': {'_name': 'sentence_prediction', 'data': 'data/cls-books-bin', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 256, 'regression_target': False, 'classification_head_name': 'sentence_classification_head', 'seed': 2}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'sentence_classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 187, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 3125.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-16 17:05:08 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 32000 types\n",
            "2023-11-16 17:05:08 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-16 17:05:09 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(32000, 512, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(258, 512, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (sentence_classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=512, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-16 17:05:09 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-16 17:05:09 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-16 17:05:09 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-16 17:05:09 | INFO | fairseq_cli.train | num. shared model params: 23,388,418 (num. trained: 23,388,418)\n",
            "2023-11-16 17:05:09 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-16 17:05:09 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/input0/valid\n",
            "2023-11-16 17:05:09 | INFO | fairseq.data.data_utils | loaded 200 examples from: data/cls-books-bin/label/valid\n",
            "2023-11-16 17:05:09 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 200\n",
            "2023-11-16 17:05:09 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/input0/test\n",
            "2023-11-16 17:05:09 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data/cls-books-bin/label/test\n",
            "2023-11-16 17:05:09 | INFO | fairseq.tasks.sentence_prediction | Loaded test with #samples: 2000\n",
            "2023-11-16 17:05:13 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-16 17:05:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:05:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-16 17:05:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-16 17:05:13 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-16 17:05:13 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 8\n",
            "2023-11-16 17:05:13 | INFO | fairseq.trainer | Preparing to load checkpoint random_model\n",
            "2023-11-16 17:05:13 | INFO | fairseq.trainer | No existing checkpoint found random_model\n",
            "2023-11-16 17:05:13 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-16 17:05:13 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/input0/train\n",
            "2023-11-16 17:05:13 | INFO | fairseq.data.data_utils | loaded 1,800 examples from: data/cls-books-bin/label/train\n",
            "2023-11-16 17:05:13 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 1800\n",
            "2023-11-16 17:05:13 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-16 17:05:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 001:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:05:13 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-16 17:05:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 223/225 [00:12<00:00, 18.72it/s, loss=0.993, nll_loss=0.007, accuracy=57.5, wps=20024.2, ups=17.8, wpb=1125.2, bsz=8, num_updates=220, lr=9.88768e-06, gnorm=5.494, train_wall=0, gb_free=14.2, wall=12]2023-11-16 17:05:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  6.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 6/25 [00:00<00:00, 27.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 12/25 [00:00<00:00, 40.13it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 19/25 [00:00<00:00, 48.51it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:05:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 1.003 | nll_loss 0.008 | accuracy 50 | wps 59479.2 | wpb 1048 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:05:26 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 001 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   0% 1/250 [00:00<00:35,  7.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   2% 6/250 [00:00<00:08, 28.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   5% 12/250 [00:00<00:05, 39.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:   7% 18/250 [00:00<00:05, 46.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  10% 24/250 [00:00<00:04, 50.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  12% 30/250 [00:00<00:04, 52.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  14% 36/250 [00:00<00:04, 50.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  17% 42/250 [00:00<00:03, 52.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  19% 48/250 [00:01<00:03, 52.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  22% 54/250 [00:01<00:03, 52.35it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  24% 60/250 [00:01<00:03, 53.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  26% 66/250 [00:01<00:03, 52.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  29% 72/250 [00:01<00:03, 52.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  31% 78/250 [00:01<00:03, 53.17it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  34% 84/250 [00:01<00:03, 52.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  36% 90/250 [00:01<00:03, 52.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  38% 96/250 [00:01<00:02, 52.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  41% 102/250 [00:02<00:02, 53.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  43% 108/250 [00:02<00:02, 52.31it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  46% 114/250 [00:02<00:02, 51.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  48% 120/250 [00:02<00:02, 50.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  50% 126/250 [00:02<00:02, 49.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 49.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 50.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  57% 143/250 [00:02<00:02, 52.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 54.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  62% 155/250 [00:03<00:01, 53.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  64% 161/250 [00:03<00:01, 54.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  67% 167/250 [00:03<00:01, 55.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  69% 173/250 [00:03<00:01, 54.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  72% 179/250 [00:03<00:01, 55.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  74% 185/250 [00:03<00:01, 56.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  76% 191/250 [00:03<00:01, 54.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  79% 198/250 [00:03<00:00, 56.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  82% 204/250 [00:03<00:00, 57.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  84% 210/250 [00:04<00:00, 56.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  87% 217/250 [00:04<00:00, 57.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  90% 224/250 [00:04<00:00, 58.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  92% 230/250 [00:04<00:00, 58.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  94% 236/250 [00:04<00:00, 57.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset:  97% 243/250 [00:04<00:00, 58.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'test' subset: 100% 250/250 [00:04<00:00, 61.46it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:05:30 | INFO | test | epoch 001 | valid on 'test' subset | loss 1.002 | nll_loss 0.008 | accuracy 50 | wps 56955.6 | wpb 1045.3 | bsz 8 | num_updates 225\n",
            "2023-11-16 17:05:30 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-16 17:05:30 | INFO | train | epoch 001 | loss 1.001 | nll_loss 0.008 | accuracy 52 | wps 14265.6 | ups 13.34 | wpb 1067.2 | bsz 8 | num_updates 225 | lr 9.87066e-06 | gnorm 4.578 | train_wall 11 | gb_free 14.3 | wall 18\n",
            "2023-11-16 17:05:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 002:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:05:30 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-16 17:05:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 224/225 [00:11<00:00, 20.92it/s, loss=1.019, nll_loss=0.01, accuracy=50, wps=19128.3, ups=23.01, wpb=831.4, bsz=8, num_updates=445, lr=9.12185e-06, gnorm=4.2, train_wall=0, gb_free=14.3, wall=29]2023-11-16 17:05:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 19.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 41.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 56.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 68.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:05:43 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.982 | nll_loss 0.007 | accuracy 55.5 | wps 73618.4 | wpb 1048 | bsz 8 | num_updates 450 | best_accuracy 55.5\n",
            "2023-11-16 17:05:43 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 002 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 16.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 43.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 53.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 59.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 62.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 59.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  18% 44/250 [00:00<00:03, 59.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  20% 50/250 [00:00<00:03, 57.80it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  22% 56/250 [00:00<00:03, 58.33it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  25% 62/250 [00:01<00:03, 54.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  27% 68/250 [00:01<00:03, 55.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  30% 74/250 [00:01<00:03, 55.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  32% 80/250 [00:01<00:03, 55.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 54.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  37% 92/250 [00:01<00:02, 55.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  39% 98/250 [00:01<00:02, 54.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  42% 104/250 [00:01<00:02, 55.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  44% 110/250 [00:01<00:02, 54.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  46% 116/250 [00:02<00:02, 55.51it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  49% 122/250 [00:02<00:02, 56.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  51% 128/250 [00:02<00:02, 56.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  54% 134/250 [00:02<00:02, 56.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  56% 140/250 [00:02<00:01, 55.60it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  58% 146/250 [00:02<00:01, 56.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 56.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 57.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  66% 164/250 [00:02<00:01, 53.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  68% 170/250 [00:03<00:01, 54.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  70% 176/250 [00:03<00:01, 56.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  73% 182/250 [00:03<00:01, 54.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  75% 188/250 [00:03<00:01, 53.76it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  78% 194/250 [00:03<00:01, 54.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  80% 200/250 [00:03<00:00, 54.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  82% 206/250 [00:03<00:00, 55.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  85% 212/250 [00:03<00:00, 55.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  87% 218/250 [00:03<00:00, 56.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  90% 224/250 [00:04<00:00, 56.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  92% 230/250 [00:04<00:00, 57.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  94% 236/250 [00:04<00:00, 57.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset:  97% 242/250 [00:04<00:00, 55.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'test' subset: 100% 250/250 [00:04<00:00, 60.49it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:05:47 | INFO | test | epoch 002 | valid on 'test' subset | loss 0.975 | nll_loss 0.007 | accuracy 56.9 | wps 59480.2 | wpb 1045.3 | bsz 8 | num_updates 450\n",
            "2023-11-16 17:05:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-16 17:05:47 | INFO | train | epoch 002 | loss 0.993 | nll_loss 0.007 | accuracy 53.7 | wps 14324.7 | ups 13.42 | wpb 1067.2 | bsz 8 | num_updates 450 | lr 9.10483e-06 | gnorm 4.593 | train_wall 11 | gb_free 14.3 | wall 35\n",
            "2023-11-16 17:05:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 003:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:05:47 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-11-16 17:05:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 223/225 [00:12<00:00, 20.27it/s, loss=0.944, nll_loss=0.006, accuracy=60, wps=24155, ups=20.44, wpb=1182, bsz=8, num_updates=670, lr=8.35602e-06, gnorm=6.605, train_wall=0, gb_free=14.3, wall=47]2023-11-16 17:06:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 17.81it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 40.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 56.60it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 67.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:06:00 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 0.942 | nll_loss 0.007 | accuracy 62.5 | wps 73632.9 | wpb 1048 | bsz 8 | num_updates 675 | best_accuracy 62.5\n",
            "2023-11-16 17:06:00 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 003 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   1% 2/250 [00:00<00:14, 17.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   4% 9/250 [00:00<00:05, 44.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   6% 16/250 [00:00<00:04, 55.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 58.80it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 64.34it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  16% 39/250 [00:00<00:03, 66.56it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  19% 47/250 [00:00<00:02, 68.50it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  22% 55/250 [00:00<00:02, 70.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  25% 63/250 [00:00<00:02, 70.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  28% 71/250 [00:01<00:02, 70.94it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  32% 79/250 [00:01<00:02, 71.35it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  35% 87/250 [00:01<00:02, 70.14it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  38% 95/250 [00:01<00:02, 69.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 69.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  44% 109/250 [00:01<00:02, 69.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  47% 117/250 [00:01<00:01, 69.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  50% 125/250 [00:01<00:01, 70.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  53% 133/250 [00:01<00:01, 70.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  56% 141/250 [00:02<00:01, 71.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  60% 149/250 [00:02<00:01, 71.41it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  63% 157/250 [00:02<00:01, 70.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  66% 165/250 [00:02<00:01, 70.05it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  69% 173/250 [00:02<00:01, 71.12it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  72% 181/250 [00:02<00:00, 71.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  76% 189/250 [00:02<00:00, 72.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  79% 197/250 [00:02<00:00, 70.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  82% 205/250 [00:02<00:00, 70.97it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  85% 213/250 [00:03<00:00, 70.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  88% 221/250 [00:03<00:00, 71.46it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  92% 229/250 [00:03<00:00, 72.22it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  95% 237/250 [00:03<00:00, 72.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'test' subset:  98% 245/250 [00:03<00:00, 71.70it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:06:04 | INFO | test | epoch 003 | valid on 'test' subset | loss 0.923 | nll_loss 0.007 | accuracy 64.1 | wps 73751.3 | wpb 1045.3 | bsz 8 | num_updates 675\n",
            "2023-11-16 17:06:04 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-11-16 17:06:04 | INFO | train | epoch 003 | loss 0.953 | nll_loss 0.007 | accuracy 60.9 | wps 14562.5 | ups 13.65 | wpb 1067.2 | bsz 8 | num_updates 675 | lr 8.33901e-06 | gnorm 4.746 | train_wall 11 | gb_free 14.2 | wall 51\n",
            "2023-11-16 17:06:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 004:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:06:04 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-11-16 17:06:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 224/225 [00:12<00:00, 21.36it/s, loss=0.803, nll_loss=0.006, accuracy=87.5, wps=24183.9, ups=20.92, wpb=1156, bsz=8, num_updates=895, lr=7.5902e-06, gnorm=6.551, train_wall=0, gb_free=14.3, wall=64]2023-11-16 17:06:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 2/25 [00:00<00:01, 18.34it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  32% 8/25 [00:00<00:00, 41.04it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 16/25 [00:00<00:00, 54.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 25/25 [00:00<00:00, 66.41it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:06:17 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 0.857 | nll_loss 0.007 | accuracy 69.5 | wps 72418.2 | wpb 1048 | bsz 8 | num_updates 900 | best_accuracy 69.5\n",
            "2023-11-16 17:06:17 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 004 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   0% 1/250 [00:00<00:27,  9.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   3% 7/250 [00:00<00:06, 37.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   6% 15/250 [00:00<00:04, 52.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:   9% 23/250 [00:00<00:03, 61.20it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  12% 31/250 [00:00<00:03, 66.24it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  15% 38/250 [00:00<00:03, 61.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  18% 46/250 [00:00<00:03, 66.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  22% 54/250 [00:00<00:02, 69.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  25% 62/250 [00:00<00:02, 70.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  28% 70/250 [00:01<00:02, 70.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  31% 78/250 [00:01<00:02, 71.12it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  34% 86/250 [00:01<00:02, 70.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 71.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  41% 102/250 [00:01<00:02, 70.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  44% 110/250 [00:01<00:01, 71.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  47% 118/250 [00:01<00:01, 71.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  50% 126/250 [00:01<00:01, 71.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  54% 134/250 [00:01<00:01, 71.86it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  57% 142/250 [00:02<00:01, 70.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  60% 150/250 [00:02<00:01, 70.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  63% 158/250 [00:02<00:01, 71.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  66% 166/250 [00:02<00:01, 70.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  70% 174/250 [00:02<00:01, 72.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  73% 182/250 [00:02<00:00, 73.13it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  76% 190/250 [00:02<00:00, 73.08it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  79% 198/250 [00:02<00:00, 73.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  82% 206/250 [00:02<00:00, 73.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  86% 214/250 [00:03<00:00, 72.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  89% 222/250 [00:03<00:00, 72.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  92% 230/250 [00:03<00:00, 72.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  95% 238/250 [00:03<00:00, 71.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'test' subset:  98% 246/250 [00:03<00:00, 73.98it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:06:20 | INFO | test | epoch 004 | valid on 'test' subset | loss 0.834 | nll_loss 0.006 | accuracy 71.2 | wps 74799 | wpb 1045.3 | bsz 8 | num_updates 900\n",
            "2023-11-16 17:06:20 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-11-16 17:06:20 | INFO | train | epoch 004 | loss 0.857 | nll_loss 0.006 | accuracy 70.7 | wps 14265.6 | ups 13.37 | wpb 1067.2 | bsz 8 | num_updates 900 | lr 7.57318e-06 | gnorm 5.078 | train_wall 11 | gb_free 14.3 | wall 68\n",
            "2023-11-16 17:06:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 225\n",
            "epoch 005:   0% 0/225 [00:00<?, ?it/s]2023-11-16 17:06:21 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-11-16 17:06:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 223/225 [00:12<00:00, 18.69it/s, loss=0.753, nll_loss=0.006, accuracy=80, wps=20195.5, ups=18.45, wpb=1094.4, bsz=8, num_updates=1120, lr=6.82437e-06, gnorm=6.266, train_wall=0, gb_free=14.3, wall=80]2023-11-16 17:06:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 1/25 [00:00<00:03,  7.05it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 5/25 [00:00<00:00, 22.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 11/25 [00:00<00:00, 36.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  68% 17/25 [00:00<00:00, 43.59it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 24/25 [00:00<00:00, 49.90it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-11-16 17:06:33 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 0.805 | nll_loss 0.006 | accuracy 71 | wps 55279.2 | wpb 1048 | bsz 8 | num_updates 1125 | best_accuracy 71\n",
            "2023-11-16 17:06:33 | INFO | fairseq_cli.train | begin validation on \"test\" subset\n",
            "\n",
            "epoch 005 | valid on 'test' subset:   0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   0% 1/250 [00:00<00:36,  6.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   2% 5/250 [00:00<00:10, 22.84it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   4% 11/250 [00:00<00:06, 36.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   7% 17/250 [00:00<00:05, 42.87it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:   9% 23/250 [00:00<00:04, 46.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  12% 29/250 [00:00<00:04, 49.08it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  14% 34/250 [00:00<00:04, 49.05it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  16% 40/250 [00:00<00:04, 50.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  18% 46/250 [00:01<00:03, 51.56it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  21% 52/250 [00:01<00:03, 52.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  23% 58/250 [00:01<00:03, 52.77it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  26% 64/250 [00:01<00:03, 51.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  28% 70/250 [00:01<00:03, 52.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  30% 76/250 [00:01<00:03, 52.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  33% 82/250 [00:01<00:03, 50.75it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  35% 88/250 [00:01<00:03, 51.07it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  38% 94/250 [00:01<00:02, 52.49it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  40% 100/250 [00:02<00:02, 52.99it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  42% 106/250 [00:02<00:02, 51.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  45% 112/250 [00:02<00:02, 52.91it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  48% 119/250 [00:02<00:02, 54.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  50% 125/250 [00:02<00:02, 55.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  52% 131/250 [00:02<00:02, 55.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  55% 137/250 [00:02<00:02, 55.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  58% 144/250 [00:02<00:01, 58.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  61% 152/250 [00:02<00:01, 62.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  64% 160/250 [00:03<00:01, 65.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  67% 168/250 [00:03<00:01, 67.46it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  70% 176/250 [00:03<00:01, 68.57it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  74% 184/250 [00:03<00:00, 70.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  77% 192/250 [00:03<00:00, 69.90it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  80% 199/250 [00:03<00:00, 69.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  83% 207/250 [00:03<00:00, 71.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  86% 215/250 [00:03<00:00, 70.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  89% 223/250 [00:03<00:00, 71.36it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  92% 231/250 [00:04<00:00, 70.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  96% 240/250 [00:04<00:00, 73.28it/s]\u001b[A\n",
            "epoch 005 | valid on 'test' subset:  99% 248/250 [00:04<00:00, 73.57it/s]\u001b[A\n",
            "                                                                         \u001b[A2023-11-16 17:06:38 | INFO | test | epoch 005 | valid on 'test' subset | loss 0.792 | nll_loss 0.006 | accuracy 73.1 | wps 62632.7 | wpb 1045.3 | bsz 8 | num_updates 1125\n",
            "2023-11-16 17:06:38 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-11-16 17:06:38 | INFO | train | epoch 005 | loss 0.73 | nll_loss 0.005 | accuracy 76.7 | wps 13965.2 | ups 13.09 | wpb 1067.2 | bsz 8 | num_updates 1125 | lr 6.80735e-06 | gnorm 6.01 | train_wall 11 | gb_free 14.3 | wall 85\n",
            "2023-11-16 17:06:38 | INFO | fairseq_cli.train | done training in 84.9 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Tensorboard Visualisation </B>\n",
        "\n",
        "In the this we will use tensorboard to visualize the training, validation and test accuracies. <b>Include and analyse in you report a screenshot of the test accuracy of the six models</b>."
      ],
      "metadata": {
        "id": "eHACXaPSLwu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/tensor.zip /content/altegrad.lab3/tensorboard_logs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id4RlXqMUvQT",
        "outputId": "34d31d6a-f386-41df-efe8-0ac60bf1812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/altegrad.lab3/tensorboard_logs/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/train/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/train/events.out.tfevents.1700154029.50d9c3947b0c.3619.3 (deflated 55%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/valid/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/valid/events.out.tfevents.1700154025.50d9c3947b0c.3619.1 (deflated 53%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/test/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/test/events.out.tfevents.1700154029.50d9c3947b0c.3619.2 (deflated 52%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/train_inner/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/2/train_inner/events.out.tfevents.1700154012.50d9c3947b0c.3619.0 (deflated 68%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/train/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/train/events.out.tfevents.1700153828.50d9c3947b0c.2530.3 (deflated 55%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/valid/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/valid/events.out.tfevents.1700153824.50d9c3947b0c.2530.1 (deflated 53%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/test/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/test/events.out.tfevents.1700153828.50d9c3947b0c.2530.2 (deflated 52%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/train_inner/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/0/train_inner/events.out.tfevents.1700153813.50d9c3947b0c.2530.0 (deflated 69%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/train/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/train/events.out.tfevents.1700153928.50d9c3947b0c.3094.3 (deflated 55%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/valid/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/valid/events.out.tfevents.1700153925.50d9c3947b0c.3094.1 (deflated 53%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/test/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/test/events.out.tfevents.1700153928.50d9c3947b0c.3094.2 (deflated 51%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/train_inner/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_ms8_mu3125_lr1e-05_me5/1/train_inner/events.out.tfevents.1700153912.50d9c3947b0c.3094.0 (deflated 69%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/train/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/train/events.out.tfevents.1700154330.50d9c3947b0c.5206.3 (deflated 55%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/valid/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/valid/events.out.tfevents.1700154326.50d9c3947b0c.5206.1 (deflated 53%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/test/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/test/events.out.tfevents.1700154330.50d9c3947b0c.5206.2 (deflated 52%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/train_inner/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/2/train_inner/events.out.tfevents.1700154314.50d9c3947b0c.5206.0 (deflated 68%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/train/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/train/events.out.tfevents.1700154129.50d9c3947b0c.4148.3 (deflated 55%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/valid/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/valid/events.out.tfevents.1700154125.50d9c3947b0c.4148.1 (deflated 53%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/test/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/test/events.out.tfevents.1700154129.50d9c3947b0c.4148.2 (deflated 52%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/train_inner/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/0/train_inner/events.out.tfevents.1700154113.50d9c3947b0c.4148.0 (deflated 69%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/train/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/train/events.out.tfevents.1700154230.50d9c3947b0c.4675.3 (deflated 55%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/valid/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/valid/events.out.tfevents.1700154226.50d9c3947b0c.4675.1 (deflated 53%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/test/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/test/events.out.tfevents.1700154230.50d9c3947b0c.4675.2 (deflated 52%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/train_inner/ (stored 0%)\n",
            "  adding: content/altegrad.lab3/tensorboard_logs/sentence_prediction/books/RoBERTa_small_fr_random_ms8_mu3125_lr1e-05_me5/1/train_inner/events.out.tfevents.1700154214.50d9c3947b0c.4675.0 (deflated 68%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 5881\n",
        "%tensorboard --logdir tensorboard_logs"
      ],
      "metadata": {
        "id": "pwVvJNExS2dl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "36d70058-1291-4e06-902c-c0ac9f108581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 2: Finetuning $RoBERTa_{small}^{fr}$ using HuggingFace's Transfromers</b>\n",
        "In this section of the lab, we will finetune a HuggingFace checkpoint of our $RoBERTa_{small}^{fr}$ on the CLS_Books dataset. Like in the first part we will start by downloading the HuggingFace checkpoint and <b>preparing a json format of the CLS_Books dataset</b> (Which is suitable for HuggingFace's checkpoints finetuning using their run_glue script)."
      ],
      "metadata": {
        "id": "PAR_P343MCKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Converting the CLS_Books dataset to json line files</b>\n",
        "\n",
        "Unlike Fairseq, you do not need to perform tokenization and binarization in Hugging Face transformer library. However, in order to use the implemented script in the transformers library, you need to convert your data to json line files (for each split: train, valid and test)\n",
        "\n",
        "for instance, each line inside you file will consist of one and one sample only, contaning the review (accessed by the key <i>sentence1</i> and its label, accessed by the key <i>label</i>. Below you can find an example from <i>valid.json</i> file.\n",
        "\n",
        "Note that these instructions are not valid for all kind of tasks. For other types of tasks (supported in Hugging face) you have to refer to their github for more details.<br>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "<i>\n",
        "{\"sentence1\":\"Seul ouvrage fran\\u00e7ais sur le th\\u00e8me Produits Structur\\u00e9s \\/ fonds \\u00e0 formule, il permet de fa\\u00e7on p\\u00e9dagogique d'appr\\u00e9hender parfaitement les m\\u00e9canismes financiers utilis\\u00e9s. Une r\\u00e9f\\u00e9rence pour ceux qui veulent comprendre les technicit\\u00e9s de base et les raisons de l'engouement des investisseurs sur ces actifs \\u00e0 hauteur de plusieurs milliards d'euros.\",\"label\":\"1\"}<br>\n",
        "{\"sentence1\":\"Livre tr\\u00e8s int\\u00e9ressant !  mais si comme moi vous cherchez des \\\"infos\\\" sur les techniques de sorties et autres \\\"modes d'emploi\\\", afin de vivre par vous m\\u00eame ce genre d'exp\\u00e9rience, c'est pas le bon livre.  \\u00e7a ne lui enl\\u00e8ve d'ailleurd rien \\u00e0 son int\\u00earet.\",\"label\":\"0\"}\n",
        "</i>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "c3M090L45oPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "SPLITS=['train', 'test', 'valid']\n",
        "\n",
        "for split in SPLITS:\n",
        "    with open('data/cls.books/'+split+'.review', 'r') as f:\n",
        "        reviews = f.readlines()\n",
        "    with open('data/cls.books/'+split+'.label', 'r') as f:\n",
        "        labels = f.readlines()\n",
        "    with open('data/cls.books-json/'+split+'.json', 'w') as f:\n",
        "\n",
        "      for review, label in zip(reviews, labels):\n",
        "        json_example = {\n",
        "          \"sentence1\": review,\n",
        "          \"label\": label.strip()\n",
        "        }\n",
        "\n",
        "        json.dump(json_example, f)\n",
        "        f.write(\"\\n\")\n",
        "        #fill the gap here to create train.json, valid.json and test.json\n"
      ],
      "metadata": {
        "id": "HZZFHEHFyv5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning $RoBERTa_{small}^{fr}$ using the Transformers Library</b>\n",
        "\n",
        "In order to finrtune the model using HuggingFace, you to use the <b>run_glue.py</b> Python script located in the transformers library. For more details, refer to <a href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\" target=\"_blank\">the Huggingface/transformers repository on Github</a>. Make sure to use the same hyperparameter as in the first part of this lab."
      ],
      "metadata": {
        "id": "ICnN2FvnhTbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "0c6vM239VsAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_SET='books'\n",
        "MODEL='RoBERTa_small_fr_huggingface'\n",
        "MAX_SENTENCES=8 # fill me, batch size.\n",
        "LR=1e-5 #fill me, learning rate\n",
        "MAX_EPOCH=5 #fill me\n",
        "NUM_CLASSES=2 #fill me\n",
        "SEEDS=3\n",
        "CUDA_VISIBLE_DEVICES=0"
      ],
      "metadata": {
        "id": "h-BBIykNjH7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for SEED in range(SEEDS):\n",
        "\n",
        "  SAVE_DIR='checkpoints/'+TASK+'/'+DATA_SET+'/'+MODEL+'_ms'+str(MAX_SENTENCES)+'_lr'+str(LR)+'_me'+str(MAX_EPOCH)+'/'+str(SEED)\n",
        "\n",
        "  !(python libs/transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "           --model_name_or_path models/RoBERTa_small_fr_HuggingFace \\\n",
        "           --task_name cls-books \\\n",
        "           --do_train \\\n",
        "           --do_eval \\\n",
        "           --max_seq_length 256 \\\n",
        "           --per_device_train_batch_size $MAX_SENTENCES \\\n",
        "           --learning_rate $LR \\\n",
        "           --num_train_epochs $MAX_EPOCH \\\n",
        "           --output_dir $SAVE_DIR \\\n",
        "           --logging_steps 10 \\\n",
        "           --evaluation_strategy steps \\\n",
        "           --load_best_model_at_end True \\\n",
        "           --metric_for_best_model accuracy \\\n",
        "           --greater_is_better True \\\n",
        "           --overwrite_output_dir \\\n",
        "           --seed $SEED \\\n",
        "         )#fill me"
      ],
      "metadata": {
        "id": "lV2Zla33hK_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir checkpoints --port 6007"
      ],
      "metadata": {
        "id": "d2UMHjatpFvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Part 3: Finetuning $BLOOM-560m$ using HuggingFace's Transfromers</b>\n",
        "In this part, we will fintune $BLOOM-560m$:https://huggingface.co/bigscience/bloom-560m on a question/answer dataset. We will equally use LoRA and quantization during the finetuning.\n",
        "\n",
        "## <b>Preparing the environment and installing libraries:<b>"
      ],
      "metadata": {
        "id": "mUCV4V0ONKeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.39.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off\n",
        "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off\n",
        "!pip install -qqq datasets==2.12.0 --progress-bar off\n",
        "!pip install -qqq loralib==0.1.1 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off"
      ],
      "metadata": {
        "id": "8-VOzEaS7Wpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "qHHXf0xHUsx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Loading the model and the tokenizer:</b>\n",
        "In this section, we will load the BLOOM model while using the BitsAndBytes library for quantization."
      ],
      "metadata": {
        "id": "MC-Kv8g8MSuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bigscience/bloom-560m\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    mode=\"quantized_only\",\n",
        "    quantization={\"method\": \"poisson\", \"bits\": 4},\n",
        "    forward_method=\"huggingface\",\n",
        ")# fill the gap\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "X6TaXDnRVKDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        # fill the gap: get the number of trainable parameters: trainable_params\n",
        "        if param.requires_grad:\n",
        "          trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "LIvuxW4lVW_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Configuring LoRA:<b>"
      ],
      "metadata": {
        "id": "UTgKyxhJMeEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "duTYSKKYVamH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Test the model before finetuning:<b>"
      ],
      "metadata": {
        "id": "5H1bBQaSNVsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<human>: Comment je peux créer un compte?  \\n <assistant>: \"\n",
        "# fill the gap, prompt of the format: \"<human>: Comment je peux créer un compte?  \\n <assistant>: \", with an empty response from the assistant\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "sRW7HPX6WCmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "DnmKXlqSWPQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Loading the question/answer dataset from HuggingFace:<b>"
      ],
      "metadata": {
        "id": "nbhQySqVMo2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"OpenLLM-France/Tutoriel\", data_files=\"ecommerce-faq-fr.json\")\n",
        "pd.DataFrame(data[\"train\"])"
      ],
      "metadata": {
        "id": "2zR54r9AWQ-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Preparing the finetuning data:<b>"
      ],
      "metadata": {
        "id": "9oSZX9UcNBsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(data_point):\n",
        "    return f\"<human>: {data_point['question']}?\\n<assistant>: {data_point['answer']}\"\n",
        "    # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: response\"\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ],
      "metadata": {
        "id": "cQiJpF41WZEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Finetuning:<b>"
      ],
      "metadata": {
        "id": "SGfbqJ_cNHDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=1,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=80,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "sjBMVb6yW_74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs --port 6008"
      ],
      "metadata": {
        "id": "B01QbSicXknK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Test the model after the finetuning:<b>"
      ],
      "metadata": {
        "id": "uxy9b1f4Nqpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "tCYynNlrXDhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question: str) -> str:\n",
        "    prompt = f\"<human>: {question} \\n <assistant>: \"\n",
        "    # fill the gap, transform the data into prompts of the format: \"<human>: question?  \\n <assistant>: \" with an empty response\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    return response[response_start + len(assistant_start) :].strip()"
      ],
      "metadata": {
        "id": "CS_lwrJdXr-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Puis-je retourner un produit s'il s'agit d'un article en liquidation ou en vente finale ?\"\n",
        "print('-', prompt,'\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "prompt = \"Que se passe-t-il lorsque je retourne un article en déstockage ?\"\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "prompt = \"Comment puis-je savoir quand je recevrai ma commande ?\"\n",
        "print(generate_response(prompt))"
      ],
      "metadata": {
        "id": "sYaO6H_hXsvG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}