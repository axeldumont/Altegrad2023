{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 1: NMT</h2><h3> Neural Machine Translation</h3> 10 / 10 / 2023<br> Dr. G. Shang and H. Abdine</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        # you should return a tensor of shape (seq, batch, feat)\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        # embedded: (seq_len, batch_size, embedding_dim)\n",
        "        output, hs = self.rnn(embedded)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "\n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = self.ff_concat(torch.cat([target_h_rep, source_hs], dim=2))\n",
        "        scores = self.ff_score(torch.tanh(concat_output)) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n",
        "        return ct, norm_scores"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        rnn_output, h = self.rnn(embedded, h)\n",
        "\n",
        "        concat_input = torch.cat([source_context, h], 2)\n",
        "\n",
        "        h_tilde= torch.tanh(self.ff_concat(concat_input))\n",
        "        prediction = self.predict(h_tilde)\n",
        "\n",
        "        return prediction, h"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "\n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "\n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "\n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "\n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod:\n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "\n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        normed_scores = []\n",
        "\n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context, norm_score_step = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "                normed_scores.append(norm_score_step.squeeze().detach().cpu())\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            target_input = prediction.argmax(dim=2) # the predicted word\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "\n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
        "        normed_scores = torch.stack(normed_scores)\n",
        "        return to_return, normed_scores\n",
        "\n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "\n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "\n",
        "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()\n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n",
        "                        pbar.set_postfix(tdqm_dict)\n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "\n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "\n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "\n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "\n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints\n",
        "\n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "\n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits, normed_scores = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl), normed_scores\n",
        "\n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "\n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5RprtnBK-ia"
      },
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "## Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "datl5SFtJ9Br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de32d400-aec8-4c9d-bc83-23f1122fd88f"
      },
      "source": [
        "### you can download the data from Moodle and upload them to Colab (data.zip and pretrained_moodle.pt)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path_to_data = '/content/drive/MyDrive/Lab1_NLP/data/data/'\n",
        "path_to_save_models = '/content/drive/MyDrive/Lab1_NLP/'"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "## Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZ-cvSuLQVt"
      },
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "\n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "\n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "\n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "\n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "\n",
        "    print('data loaded')\n",
        "\n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "\n",
        "    print('data prepared')\n",
        "\n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "\n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "\n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "## Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "UCvZmwWoCTUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e50f2f-0d11-4a87-e7ef-d7704fe3d9b5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhXbQjP_YrgY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48989a0b-b52e-4a3b-82f3-3d344ce377bc"
      },
      "source": [
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "\n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "\n",
        "    for i,elt in enumerate(to_test):\n",
        "      translation, attention = model.predict(elt)\n",
        "      if i==0:\n",
        "            legend_elt = elt.split(' ')\n",
        "            legend_translation = translation.split(' ')[0:3]\n",
        "\n",
        "            plt.figure()\n",
        "            fig, ax = plt.subplots()\n",
        "            im = ax.imshow(attention.numpy()[1:4,1::], cmap='Greys_r')\n",
        "            ax.set_xticks(np.arange(len(legend_elt)))\n",
        "            ax.set_yticks(np.arange(len(legend_translation)))\n",
        "            ax.set_xticklabels(legend_elt)\n",
        "            ax.set_yticklabels(legend_translation)\n",
        "\n",
        "            plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
        "            rotation_mode=\"anchor\")\n",
        "            plt.show()\n",
        "      if i==1:\n",
        "            legend_elt = elt.split(' ')\n",
        "            legend_translation = translation.split(' ')[0:5]\n",
        "\n",
        "            plt.figure()\n",
        "            fig, ax = plt.subplots()\n",
        "            im = ax.imshow(attention.numpy()[1:6,1::], cmap='Greys_r')\n",
        "            ax.set_xticks(np.arange(len(legend_elt)))\n",
        "            ax.set_yticks(np.arange(len(legend_translation)))\n",
        "            ax.set_xticklabels(legend_elt)\n",
        "            ax.set_yticklabels(legend_translation)\n",
        "\n",
        "            plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
        "            rotation_mode=\"anchor\")\n",
        "            plt.show()\n",
        "      print('= = = = = \\n','%s -> %s' % (elt, translation))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "['I', 'am', 'a', 'student.']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHICAYAAACvevVkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbGElEQVR4nO3deYxVhfn44ffOCCPqDCCCgo4sLgiIoDEirtWi1roEMWparRIV26hxNyk1KmhlGrfiWre0bmlsxS1KW+qGIKWocaQYV1yA6FgVKwNiWWbu9w9/zq9TBITKHOB9nmQS7znnXt7LVeeTs9xTKpfL5QAASKyi6AEAAIomiACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpbVL0ABui5ubm+PDDD6O6ujpKpVLR4wAAK1Eul2PhwoXRo0ePqKhY+X4gQbQWPvzww6itrS16DADgW5o3b15st912K10viNZCdXV1RES8/PLLscUWWxQ8DevanXfeWfQItJE5c+YUPQJt6Jlnnil6BNpAc3NzfPbZZy2/u1dGEK2Frw+TbbHFFqv9C2bDV1VVVfQItJF27doVPQJtaFWHT9j4rO4UF/82AADpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANJLE0QjR46M4cOHFz0GALAe2qToAdrKDTfcEOVyuegxAID1UJog6tixY9EjAADrqZSHzJqbm6Ouri569+4dHTp0iEGDBsWECROKHRAAKEyaPUT/qa6uLu6///647bbbYqeddoopU6bESSedFF27do0DDzxwhe2XLFkSS5YsaXnc2NjYluMCAOtYuiBasmRJjBs3Lp566qkYOnRoRET06dMnnn/++bj99tu/MYjq6upi7NixbT0qANBG0gXR7NmzY/HixXHIIYe0Wr506dLYfffdv/E5o0ePjgsuuKDlcWNjY9TW1q7TOQGAtpMuiBYtWhQRERMnToxtt9221bqqqqpvfE5VVdVK1wEAG750QdS/f/+oqqqKuXPnfuPhMQAgn3RBVF1dHRdddFGcf/750dzcHPvtt18sWLAgpk2bFjU1NXHKKacUPSIA0MbSBVFExJVXXhldu3aNurq6ePfdd6NTp06xxx57xC9+8YuiRwMACpAmiJYsWRJbbLFFRESUSqU499xz49xzzy14KgBgfbDRfzHj8uXL47XXXovp06fHgAEDih4HAFgPbfRB9Oqrr8aee+4ZAwYMiJ/97GdFjwMArIc2+kNmgwcPjsWLFxc9BgCwHtvo9xABAKyOIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6pXK5XC56iA1NY2NjdOzYMdq1axelUqnocVjHxo8fX/QItJHjjz++6BFoQ6NHjy56BNrA0qVL45577okFCxZETU3NSrezhwgASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAemmCqFevXjF+/PiixwAA1kObFD1AW3nxxRdj8803L3oMAGA9lCaIunbtWvQIAMB6aoM6ZDZhwoQYOHBgdOjQIbp06RLDhg2LL774Ir73ve/Feeed12rb4cOHx8iRI1se/+chs3K5HGPGjIntt98+qqqqokePHnHOOee03RsBANYrG8weooaGhvjRj34UV199dRxzzDGxcOHCmDp1apTL5TV+rYceeih+/etfxwMPPBADBgyIjz76KGbOnLnS7ZcsWRJLlixpedzY2LhW7wEAWD9tUEG0fPnyGDFiRPTs2TMiIgYOHLhWrzV37tzYZpttYtiwYdGuXbvYfvvtY6+99lrp9nV1dTF27Ni1+rMAgPXfBnPIbNCgQfH9738/Bg4cGMcdd1zceeed8a9//WutXuu4446LL7/8Mvr06ROjRo2KRx55JJYvX77S7UePHh0LFixo+Zk3b97avg0AYD20wQRRZWVlPPnkk/HnP/85+vfvHzfddFP07ds33nvvvaioqFjh0NmyZctW+lq1tbXx5ptvxq233hodOnSIM888Mw444ICVPqeqqipqampa/QAAG48NJogiIkqlUuy7774xduzYqK+vj/bt28cjjzwSXbt2jYaGhpbtmpqa4tVXX13la3Xo0CGOOuqouPHGG2Py5Mkxffr0mDVr1rp+CwDAemiDOYdoxowZ8fTTT8ehhx4a3bp1ixkzZsQnn3wS/fr1i8033zwuuOCCmDhxYuywww5x/fXXx+eff77S17r77rujqakphgwZEptttlncf//90aFDh5ZzkwCAXDaYIKqpqYkpU6bE+PHjo7GxMXr27BnXXXddHH744bFs2bKYOXNmnHzyybHJJpvE+eefHwcddNBKX6tTp07xq1/9Ki644IJoamqKgQMHxuOPPx5dunRpw3cEAKwvSuW1uW49ucbGxujYsWO0a9cuSqVS0eOwjrnlSx7HH3980SPQhkaPHl30CLSBpUuXxj333BMLFixY5TnAG9Q5RAAA64IgAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgvVK5XC4XPcSGprGxMTp27Fj0GMB3rL6+vugRaEODBw8uegTawNe/sxcsWBA1NTUr3c4eIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDp/c9BdMMNN8T06dO/i1kAAArxPwXRddddFw8//HDsscce39U8rZRKpXj00UcjIuL999+PUqkUr7zyyjr5swCAvNY6iKZNmxb33XdfPPbYY1FVVRUREZMnT45SqRSff/75dzVfi9ra2mhoaIhdd931O33dXr16xfjx47/T1wQANiybrO0T99133zbdW1NZWRnbbLNNm/15AEAea7yHqLm5Oerq6qJ3797RoUOHGDRoUEyYMCHef//9OOiggyIionPnzlEqlWLkyJER8c17YQYPHhxjxoxpefz222/HAQccEJtuumn0798/nnzyyVbb//chs6ampjjttNNa5ujbt2/ccMMNrZ4zcuTIGD58eFx77bXRvXv36NKlS5x11lmxbNmyiIj43ve+F3PmzInzzz8/SqVSlEqlb3zPS5YsicbGxlY/AMDGY433ENXV1cX9998ft912W+y0004xZcqUOOmkk2LSpEnx0EMPxbHHHhtvvvlm1NTURIcOHb7VazY3N8eIESNi6623jhkzZsSCBQvivPPOW+1ztttuu3jwwQejS5cu8be//S3OOOOM6N69exx//PEt2z377LPRvXv3ePbZZ2P27NlxwgknxODBg2PUqFHx8MMPx6BBg+KMM86IUaNGrfI9jx079lu9FwBgw7NGQbRkyZIYN25cPPXUUzF06NCIiOjTp088//zzcfvtt8cZZ5wRERHdunWLTp06fevXfeqpp+KNN96ISZMmRY8ePSIiYty4cXH44Yev9Dnt2rVrFSm9e/eO6dOnxx//+MdWQdS5c+e4+eabo7KyMnbZZZc44ogj4umnn45Ro0bFlltuGZWVlVFdXb3Kw3GjR4+OCy64oOVxY2Nj1NbWfuv3BwCs39YoiGbPnh2LFy+OQw45pNXypUuXxu67777WQ7z++utRW1vbEkMR0RJcq3LLLbfEb3/725g7d258+eWXsXTp0hg8eHCrbQYMGBCVlZUtj7t37x6zZs1ao/mqqqpaThwHADY+axREixYtioiIiRMnxrbbbttqXVVVVbzzzjvf+LyKioool8utln19Hs/aeuCBB+Kiiy6K6667LoYOHRrV1dVxzTXXxIwZM1pt165du1aPS6VSNDc3/09/NgCwcVmjIOrfv39UVVXF3Llz48ADD1xh/bx58yLiqxOe/1PXrl2joaGh5XFjY2O89957LY/79esX8+bNi4aGhujevXtERPz9739f5SzTpk2LffbZJ84888yWZSsLslVp3779CvMCALms0VVm1dXVcdFFF8X5558f99xzT7zzzjvx8ssvx0033RT33HNP9OzZM0qlUjzxxBPxySeftOxROvjgg+O+++6LqVOnxqxZs+KUU05pdRhr2LBhsfPOO8cpp5wSM2fOjKlTp8Yll1yyyll22mmneOmll2LSpEnx1ltvxaWXXhovvvjiGv8F9OrVK6ZMmRIffPBBfPrpp2v8fABgw7fGl91feeWVcemll0ZdXV3069cvfvCDH8TEiROjd+/ese2228bYsWPj5z//eWy99dZx9tlnR8RXJyUfeOCBceSRR8YRRxwRw4cPjx122OH/D1FREY888kh8+eWXsddee8Xpp58eV1111Srn+OlPfxojRoyIE044IYYMGRLz589vtbfo27riiivi/fffjx122CG6du26xs8HADZ8pfJ/n9zDajU2NkbHjh2LHgP4jtXX1xc9Am3ovy/CYeP09e/sBQsWRE1NzUq3c7d7ACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKQniACA9AQRAJCeIAIA0hNEAEB6gggASE8QAQDpCSIAID1BBACkJ4gAgPQEEQCQniACANITRABAeoIIAEhPEAEA6QkiACA9QQQApCeIAID0BBEAkJ4gAgDS26ToATZE5XK56BGAdWDRokVFj0AbamxsLHoE2sDXn/PqfncLorWwcOHCokcA1oH999+/6BGAdWThwoXRsWPHla4vle3uWGPNzc3x4YcfRnV1dZRKpaLHaTONjY1RW1sb8+bNi5qamqLHYR3yWefhs84j62ddLpdj4cKF0aNHj6ioWPmZQvYQrYWKiorYbrvtih6jMDU1Nan+Y8rMZ52HzzqPjJ/1qvYMfc1J1QBAeoIIAEhPEPGtVVVVxeWXXx5VVVVFj8I65rPOw2edh8961ZxUDQCkZw8RAJCeIAIA0hNEAEB6gggASE8QAQDp+aZqAFbw2muvxdy5c2Pp0qWtlh999NEFTQTrliBijY0YMeJbbffwww+v40loK/Pnz4/LLrssnn322fj444+jubm51frPPvusoMn4rr377rtxzDHHxKxZs6JUKrXcIfzr+zY2NTUVOR6sM4KINfZt7gnDxuUnP/lJzJ49O0477bTYeuutU93UOJtzzz03evfuHU8//XT07t07XnjhhZg/f35ceOGFce211xY9Hm1g2LBh8e6778a7775b9ChtyhczAqtVXV0dzz//fAwaNKjoUVjHttpqq3jmmWdit912i44dO8YLL7wQffv2jWeeeSYuvPDCqK+vL3pE1rFbbrklPv3007j88suLHqVN2UMErNYuu+wSX375ZdFj0Aaampqiuro6Ir6Kow8//DD69u0bPXv2jDfffLPg6WgLZ511VtEjFMJVZsBq3XrrrXHJJZfEc889F/Pnz4/GxsZWP2w8dt1115g5c2ZERAwZMiSuvvrqmDZtWlxxxRXRp0+fgqfju3TqqafGwoULV1j+xRdfxKmnnlrARMVyyAxYrbfffjt+/OMfx8svv9xqeblcjlKp5ETbjcikSZPiiy++iBEjRsTs2bPjyCOPjLfeeiu6dOkSf/jDH+Lggw8uekS+I5WVldHQ0BDdunVrtfzTTz+NbbbZJpYvX17QZMVwyAxYrRNPPDHatWsXv//9751UvZE77LDDWv55xx13jDfeeCM+++yz6Ny5s899I9HY2BjlcjnK5XIsXLgwNt1005Z1TU1N8ac//WmFSMpAEAGr9eqrr0Z9fX307du36FEowJZbbln0CHyHOnXqFKVSKUqlUuy8884rrC+VSjF27NgCJiuWIAJWa88994x58+YJItgIPPvss1Eul+Pggw+Ohx56qFXwtm/fPnr27Bk9evQocMJiOIcIWK0HH3wwxowZExdffHEMHDgw2rVr12r9brvtVtBkwNqaM2dO1NbWRkWF66siBBHwLazqf5hOqoYN1+effx4vvPDCN34D/cknn1zQVMUQRMBqzZkzZ5Xre/bs2UaTAN+Vxx9/PE488cRYtGhR1NTUtDppvlQqpbsljyACvrVvuuFnqVSKo446qsCpgLWx8847xw9/+MMYN25cbLbZZkWPUzhBBKyWG37CxmfzzTePWbNm+cLN/8eZVMBqfX3Dz48//jg222yzePXVV2PKlCmx5557xuTJk4seD1gLhx12WLz00ktFj7HecNk9sFrTp0+PZ555JrbaaquoqKiIysrK2G+//aKuri7OOeccN/yEDdARRxwRF198cbz22mvfePXo0UcfXdBkxXDIDFitzp07x8svvxy9e/eOHXbYIe6666446KCD4p133omBAwfG4sWLix4RWEOuHm3NHiJgtb6+4Wfv3r1bbvjZvn37uOOOO5x/ABuo/77MPjt7iIDVcsNP2Lj9+9//bnVPs4wEEbBW3PATNmxNTU0xbty4uO222+Kf//xnvPXWW9GnT5+49NJLo1evXnHaaacVPWKbcpUZsFa23HJLMQQbsKuuuiruvvvulkPgX9t1113jrrvuKnCyYggiAEjo3nvvjTvuuCNOPPHEqKysbFk+aNCgeOONNwqcrBiCCAAS+uCDD2LHHXdcYXlzc3MsW7asgImKJYgAIKH+/fvH1KlTV1g+YcKE2H333QuYqFguuweAhC677LI45ZRT4oMPPojm5uZ4+OGH480334x77703nnjiiaLHa3OuMgOApKZOnRpXXHFFzJw5MxYtWhR77LFHXHbZZXHooYcWPVqbE0QAQHrOIQIA0nMOEQAksSZfpvrZZ5+t42nWL4IIAJIYP358yz/Pnz8/fvnLX8Zhhx0WQ4cOjYiI6dOnx6RJk+LSSy8taMLiOIcIABI69thj46CDDoqzzz671fKbb745nnrqqXj00UeLGawggggAEtpiiy3ilVdeWeHLGWfPnh2DBw+ORYsWFTRZMZxUDQAJdenSJR577LEVlj/22GPRpUuXAiYqlnOIACChsWPHxumnnx6TJ0+OIUOGRETEjBkz4i9/+UvceeedBU/X9hwyA4CkZsyYETfeeGO8/vrrERHRr1+/OOecc1oCKRNBBACk55AZACQ0d+7cVa7ffvvt22iS9YM9RACQUEVFxSq/pLGpqakNpymePUQAkFB9fX2rx8uWLYv6+vq4/vrr46qrripoquLYQwQAtJg4cWJcc801MXny5KJHaVO+hwgAaNG3b9948cUXix6jzTlkBgAJNTY2tnpcLpejoaEhxowZEzvttFNBUxVHEAFAQp06dVrhpOpyuRy1tbXxwAMPFDRVcZxDBAAJPffcc60eV1RURNeuXWPHHXeMTTbJt78k3zsGAKJUKsU+++yzQvwsX748pkyZEgcccEBBkxXDHiIASKiysjIaGhqiW7durZbPnz8/unXrlu57iFxlBgAJlcvlb/xixvnz58fmm29ewETFcsgMABIZMWJERHx1yGzkyJFRVVXVsq6pqSn+8Y9/xD777FPUeIURRACQSMeOHSPiqz1E1dXV0aFDh5Z17du3j7333jtGjRpV1HiFEUQAkMjvfve7iIjo2rVrjBkzJjbbbLOIiHj//ffj0UcfjX79+sVWW21V5IiFcA4RACRUX18f9957b0REfP7557H33nvHddddF8OHD4/f/OY3BU/X9gQRACRUX18f+++/f0RETJgwIbbeeuuYM2dO3HvvvXHjjTcWPF3bE0QAkNDixYujuro6IiL++te/xogRI6KioiL23nvvmDNnTsHTtT1BBAAJ7bjjjvHoo4/GvHnzYtKkSXHooYdGRMTHH38cNTU1BU/X9gQRACR02WWXxUUXXRS9evWKIUOGxNChQyPiq71Fu+++e8HTtT3fVA0ASX300UfR0NAQgwYNioqKr/aRvPDCC1FTUxO77LJLwdO1LUEEAKTnkBkAkJ4gAgDSE0QAQHqCCABITxABAOkJIgAgPUEEAKT3f8nCY4wfcqm1AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= = = = = \n",
            " I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "['I', 'have', 'a', 'red', 'car.']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= = = = = \n",
            " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean -> elle est tellement méchant méchant . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas empêcher de de fumer fumer fumer fumer fumer fumer fumer fumer fumer fumer urgence urgence urgence urgence urgence urgence . urgence urgence . urgence urgence .\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants jouent cache cache cache cache caché caché caché caché caché caché caché caché caché caché caché caché caché caché caché dentifrice perdre caché risques rapide caché risques éveillés\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat s est en du du pression peigne peigne cheminée portail portail portail portail portail portail portail portail indépendant oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44fKMCLsbNvr"
      },
      "execution_count": 70,
      "outputs": []
    }
  ]
}